"use strict";(self.webpackChunknumpower=self.webpackChunknumpower||[]).push([[7164],{3905:(e,t,r)=>{r.d(t,{Zo:()=>c,kt:()=>h});var n=r(67294);function a(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function o(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,n)}return r}function i(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?o(Object(r),!0).forEach((function(t){a(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):o(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function s(e,t){if(null==e)return{};var r,n,a=function(e,t){if(null==e)return{};var r,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||(a[r]=e[r]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)r=o[n],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var l=n.createContext({}),u=function(e){var t=n.useContext(l),r=t;return e&&(r="function"==typeof e?e(t):i(i({},t),e)),r},c=function(e){var t=u(e.components);return n.createElement(l.Provider,{value:t},e.children)},p="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},m=n.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=u(r),m=a,h=p["".concat(l,".").concat(m)]||p[m]||d[m]||o;return r?n.createElement(h,i(i({ref:t},c),{},{components:r})):n.createElement(h,i({ref:t},c))}));function h(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,i=new Array(o);i[0]=m;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[p]="string"==typeof e?e:a,i[1]=s;for(var u=2;u<o;u++)i[u]=r[u];return n.createElement.apply(null,i)}return n.createElement.apply(null,r)}m.displayName="MDXCreateElement"},4737:(e,t,r)=>{r.r(t),r.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>d,frontMatter:()=>o,metadata:()=>s,toc:()=>u});var n=r(87462),a=(r(67294),r(3905));const o={},i="Losses",s={unversionedId:"nn/losses",id:"nn/losses",title:"Losses",description:"Loss functions are used to measure how well a model's predictions match the actual outcomes, providing a value that the model aims to minimize during the training process.",source:"@site/tensor/nn/0-losses.mdx",sourceDirName:"nn",slug:"/nn/losses",permalink:"/tensor/nn/losses",draft:!1,tags:[],version:"current",sidebarPosition:0,frontMatter:{},sidebar:"apiSidebar",previous:{title:"Neural Network Module",permalink:"/tensor/category/neural-network-module"},next:{title:"Activations",permalink:"/tensor/nn/activations"}},l={},u=[{value:"Regression",id:"regression",level:2},{value:"MeanSquaredError",id:"meansquarederror",level:3},{value:"MeanAbsoluteError",id:"meanabsoluteerror",level:3},{value:"Probabilistic",id:"probabilistic",level:2},{value:"BinaryCrossEntropy",id:"binarycrossentropy",level:3}],c={toc:u},p="wrapper";function d(e){let{components:t,...r}=e;return(0,a.kt)(p,(0,n.Z)({},c,r,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"losses"},"Losses"),(0,a.kt)("p",null,"Loss functions are used to measure how well a model's predictions match the actual outcomes, providing a value that the model aims to minimize during the training process."),(0,a.kt)("p",null,"Loss functions can be found through static calls in the class ",(0,a.kt)("inlineCode",{parentName:"p"},"NumPower\\NeuralNetwork\\Losses;"),"."),(0,a.kt)("h2",{id:"regression"},"Regression"),(0,a.kt)("hr",null),(0,a.kt)("p",null,"Regression losses are used to evaluate the performance of regression models, which predict continuous values.\nThese losses quantify the difference between the predicted values generated by the model and the actual\nvalues from the data. The goal during training is to minimize these losses, thereby improving the model's accuracy."),(0,a.kt)("h3",{id:"meansquarederror"},"MeanSquaredError"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"public static function MeanSquaredError(int|float|array|\\NDArray|Tensor $x,\n                                        int|float|array|\\NDArray|Tensor $y,\n                                        ?string                $reduction = 'mean',\n                                        string                 $name = ''): Tensor\n")),(0,a.kt)("p",null,"Calculates the Mean Squared Error (MSE) between two inputs, ",(0,a.kt)("inlineCode",{parentName:"p"},"$x")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"$y"),". This function is a key metric for\nevaluating the performance of regression models. The MSE is computed by averaging the squared differences\nbetween the predicted and actual values."),(0,a.kt)("p",null,"An optional ",(0,a.kt)("inlineCode",{parentName:"p"},"$reduction")," parameter can be specified to control how the final result is aggregated. By default, it is\nset to ",(0,a.kt)("inlineCode",{parentName:"p"},"mean"),", which returns the average of all squared errors. Another common option is ",(0,a.kt)("inlineCode",{parentName:"p"},"sum"),",\nwhich returns the total sum of all squared errors."),(0,a.kt)("h3",{id:"meanabsoluteerror"},"MeanAbsoluteError"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"public static function MeanAbsoluteError(int|float|array|\\NDArray|Tensor $x,\n                                         int|float|array|\\NDArray|Tensor $y,\n                                         ?string                $reduction = 'mean',\n                                         string                 $name = ''): Tensor\n")),(0,a.kt)("p",null,"Calculates the Mean Absolute Error (MAE) between two inputs, $x and $y. This function is essential for assessing the\naccuracy of regression models by measuring the average magnitude of the errors in a set of predictions, without\nconsidering their direction. The MAE is computed by averaging the absolute differences between the\npredicted and actual values."),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"$reduction")," parameter allows customization of how the final result is summarized.\nBy default, it is set to ",(0,a.kt)("inlineCode",{parentName:"p"},"mean"),", which returns the average of all absolute errors, but ",(0,a.kt)("inlineCode",{parentName:"p"},"sum"),"\ncan also be specified to obtain the total sum of all absolute errors."),(0,a.kt)("h2",{id:"probabilistic"},"Probabilistic"),(0,a.kt)("hr",null),(0,a.kt)("p",null,"Probabilistic losses are used in models that predict probability distributions over outcomes rather than single\npoint estimates. These losses measure how well the predicted probability distribution aligns with the\nactual distribution of the data. The goal is to minimize these losses to improve the model's\nability to predict accurate probabilities."),(0,a.kt)("h3",{id:"binarycrossentropy"},"BinaryCrossEntropy"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"public static function BinaryCrossEntropy(int|float|array|\\NDArray|Tensor $x,\n                                          int|float|array|\\NDArray|Tensor $y,\n                                          ?string                $reduction = 'mean',\n                                          string                 $name = ''): Tensor\n")),(0,a.kt)("p",null,"Computes the Binary Cross Entropy loss between the inputs ",(0,a.kt)("inlineCode",{parentName:"p"},"$x")," (predictions) and ",(0,a.kt)("inlineCode",{parentName:"p"},"$y")," (targets). This function is commonly\nused in binary classification tasks to measure the difference between probability distributions of\npredicted and actual class labels."),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"$reduction")," parameter specifies how the final loss should be aggregated. The default is ",(0,a.kt)("inlineCode",{parentName:"p"},"mean"),",\nwhich returns the average loss across all samples. Alternatively, ",(0,a.kt)("inlineCode",{parentName:"p"},"sum")," can be\nspecified to return the total sum of losses."))}d.isMDXComponent=!0}}]);