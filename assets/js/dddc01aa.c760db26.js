"use strict";(self.webpackChunknumpower=self.webpackChunknumpower||[]).push([[540],{3905:(e,n,t)=>{t.d(n,{Zo:()=>u,kt:()=>h});var a=t(67294);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){r(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},o=Object.keys(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)t=o[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=a.createContext({}),p=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},u=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},d="mdxType",c={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,r=e.mdxType,o=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=p(t),m=r,h=d["".concat(s,".").concat(m)]||d[m]||c[m]||o;return t?a.createElement(h,i(i({ref:n},u),{},{components:t})):a.createElement(h,i({ref:n},u))}));function h(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=m;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[d]="string"==typeof e?e:r,i[1]=l;for(var p=2;p<o;p++)i[p]=t[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},98651:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>p});var a=t(87462),r=(t(67294),t(3905));const o={},i="Basic usage",l={unversionedId:"intro/basic-usage",id:"intro/basic-usage",title:"Basic usage",description:"On this page we will see how we can use the Autograd library in a simplified way and",source:"@site/tensor/intro/3-basic-usage.mdx",sourceDirName:"intro",slug:"/intro/basic-usage",permalink:"/tensor/intro/basic-usage",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{},sidebar:"apiSidebar",previous:{title:"Installing Autograd",permalink:"/tensor/intro/installing"},next:{title:"Simple Neural Net from scratch using Autograd",permalink:"/tensor/intro/autograd-model"}},s={},p=[{value:"The Tensor class",id:"the-tensor-class",level:2},{value:"Tensor naming and graphs",id:"tensor-naming-and-graphs",level:3},{value:"Calculating gradients",id:"calculating-gradients",level:3},{value:"Section source code",id:"section-source-code",level:3},{value:"Using the GPU to perform operations",id:"using-the-gpu-to-perform-operations",level:2}],u={toc:p},d="wrapper";function c(e){let{components:n,...t}=e;return(0,r.kt)(d,(0,a.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"basic-usage"},"Basic usage"),(0,r.kt)("p",null,"On this page we will see how we can use the Autograd library in a simplified way and\nhow to use a GPU to perform mathematical operations."),(0,r.kt)("h2",{id:"the-tensor-class"},"The Tensor class"),(0,r.kt)("p",null,"A Tensor is a fundamental data structure used in machine learning and deep learning to represent\nmultidimensional arrays. Tensors generalize matrices to higher dimensions.\nThey are essentially a way to store data in a format that can be efficiently processed by various\nmathematical operations and are crucial in the implementation of neural networks and other machine learning algorithms."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},'<?php\nrequire_once "vendor/autoload.php";\n\nuse \\NumPower\\Tensor;\n\n$x = new Tensor([[1, 2], [3, 4]]);\n$y = new Tensor([[5, 6], [7, 8]]);\n')),(0,r.kt)("p",null,"In the code above we created two tensors, and now we can perform\nseveral high-performance mathematical operations, let's do some mathematical operations:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$result = ($x + $y) / $y;\necho $result;\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"[[1.2, 1.33333]\n [1.42857, 1.5]]\n")),(0,r.kt)("h3",{id:"tensor-naming-and-graphs"},"Tensor naming and graphs"),(0,r.kt)("p",null,"Before calculating the derivatives, let's call the graph method so we can visualize our\nbackward pass, this step is optional and is generally used for debugging:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$result->graph();\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Operation            Arguments\n==================== ========================================\ndivide               [_nd_, _nd_]\nadd                  [_nd_, _nd_]\n")),(0,r.kt)("p",null,"We can see that our operations were properly registered, but as our tensors are not named, they only appear as ",(0,r.kt)("inlineCode",{parentName:"p"},"_nd_"),"\nin the arguments. To make the graph and other parts of your code easier to read, we can name our tensors:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$x = new Tensor([[1, 2], [3, 4]], name: 'x');\n$y = new Tensor([[5, 6], [7, 8]], name: 'y');\n")),(0,r.kt)("p",null,"Now when we call the ",(0,r.kt)("inlineCode",{parentName:"p"},"graph()")," method we will be able to visualize more clearly our forward pass (from bottom to top)\nand our backward pass (from top to bottom) where the derivatives will be calculated."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Operation            Arguments\n==================== ========================================\ndivide               [x, y]\nadd                  [x, y]\n")),(0,r.kt)("p",null,"The output tensors will inherit the names of the input tensors in some operations, so ",(0,r.kt)("inlineCode",{parentName:"p"},"x")," in the division\noperation is actually the result of the sum of ",(0,r.kt)("inlineCode",{parentName:"p"},"x + y"),". You override this name in operations when specifying a\nname for the output of an operation using the ",(0,r.kt)("inlineCode",{parentName:"p"},"name")," argument"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$result = $x->add($y, name: 'out_add') / $y;\n")),(0,r.kt)("p",null,"Now if we run the code again, we will have the following graph:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Operation            Arguments\n==================== ========================================\ndivide               [out_add, y]\nadd                  [x, y]\n")),(0,r.kt)("p",null,"Great, now we have a clear view of our graph and an overview of our forward and backward pass."),(0,r.kt)("h3",{id:"calculating-gradients"},"Calculating gradients"),(0,r.kt)("p",null,"Finally, let's now calculate the gradients of ",(0,r.kt)("inlineCode",{parentName:"p"},"$result")," with respect to ",(0,r.kt)("inlineCode",{parentName:"p"},"$x")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"$y"),". To do this,\nsimply call the ",(0,r.kt)("inlineCode",{parentName:"p"},"backward()")," method in the ",(0,r.kt)("inlineCode",{parentName:"p"},"$result")," variable."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$result->backward();\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Fatal error: Uncaught Exception: grad can only be created for scalar outputs\n")),(0,r.kt)("p",null,"Oops! We have an exception. This happens because the Autograd library requires the gradient to be calculated\nfrom a scalar and in this case, ",(0,r.kt)("inlineCode",{parentName:"p"},"$result")," is an n-dimensional array. To do this, we need to reduce our\noutput using a method like ",(0,r.kt)("inlineCode",{parentName:"p"},"sum")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"mean"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$result = ($x->add($y, name: 'out_add') / $y)->sum();\n")),(0,r.kt)("p",null,"Now that we have a scalar, when we call Tensor's ",(0,r.kt)("inlineCode",{parentName:"p"},"backward()")," method the derivatives can be calculated.\nTo get the gradients of ",(0,r.kt)("inlineCode",{parentName:"p"},"$x")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"$y"),", just use the ",(0,r.kt)("inlineCode",{parentName:"p"},"grad()")," method on both variables after calling the ",(0,r.kt)("inlineCode",{parentName:"p"},"backward")," method:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$result->backward();\necho $x->grad();\necho $y->grad();\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Fatal error: Uncaught Exception: No gradient found for `x`\n")),(0,r.kt)("p",null,"Another exception, but now our problem is that we do not specify which tensors should have their gradients calculated.\nBy default, tensors do not require gradient calculation, this is to avoid calculating and storing useless gradients in more complex\nfunctions. We will use the ",(0,r.kt)("inlineCode",{parentName:"p"},"requireGrad")," argument when creating ",(0,r.kt)("inlineCode",{parentName:"p"},"$x")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"$y")," to say that we would\nlike to calculate the gradients for both input tensors:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$x = new Tensor([[1, 2], [3, 4]], name: 'x', requireGrad: True);\n$y = new Tensor([[5, 6], [7, 8]], name: 'y', requireGrad: True);\n")),(0,r.kt)("p",null,"Now when we re-execute the code, we will have the gradients of both inputs in relation to ",(0,r.kt)("inlineCode",{parentName:"p"},"$result"),":"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"[[0.2, 0.166667]\n [0.142857, 0.125]]\n[[-0.04, -0.0555556]\n [-0.0612245, -0.0625]]\n")),(0,r.kt)("h3",{id:"section-source-code"},"Section source code"),(0,r.kt)("p",null,"Here is what your code should look like when you finish this section:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"<?php\nrequire_once \"vendor/autoload.php\";\nuse NumPower\\Tensor;\n\n$x = new Tensor([[1, 2], [3, 4]], name: 'x', requireGrad: True);\n$y = new Tensor([[5, 6], [7, 8]], name: 'y', requireGrad: True);\n\n$result = ($x->add($y, name: 'out_add') / $y)->sum();\n\n$result->backward();\necho $x->grad();\necho $y->grad();\n")),(0,r.kt)("h2",{id:"using-the-gpu-to-perform-operations"},"Using the GPU to perform operations"),(0,r.kt)("p",null,"If you have a graphics card with CUDA support and have compiled and installed the NumPower extension with\nGPU support enabled, you can use your GPU to store and perform mathematical and manipulation operations."),(0,r.kt)("p",null,"In our implementation, we thought about practicality and therefore, to use your GPU, simply use the\n",(0,r.kt)("inlineCode",{parentName:"p"},"useGpu")," argument when creating your Tensor."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-php"},"$x = new Tensor([[1, 2], [3, 4]], name: 'x', requireGrad: True, useGpu: True);\n$y = new Tensor([[5, 6], [7, 8]], name: 'y', requireGrad: True, useGpu: True);\n")),(0,r.kt)("blockquote",null,(0,r.kt)("p",{parentName:"blockquote"},(0,r.kt)("strong",{parentName:"p"},"Device Mismatch"),": Like NDArrays, tensors also require that all tensors involved in operations with multiple\narguments are stored on the same device. Operations between tensors stored on the CPU and\nothers stored on the GPU will cause a fatal error.")))}c.isMDXComponent=!0}}]);