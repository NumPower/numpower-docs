"use strict";(self.webpackChunknumpower=self.webpackChunknumpower||[]).push([[8666],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>m});var r=n(67294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);t&&(r=r.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,r)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,r,a=function(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var l=r.createContext({}),u=function(e){var t=r.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},d=function(e){var t=u(e.components);return r.createElement(l.Provider,{value:t},e.children)},p="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return r.createElement(r.Fragment,{},t)}},c=r.forwardRef((function(e,t){var n=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),p=u(n),c=a,m=p["".concat(l,".").concat(c)]||p[c]||h[c]||i;return n?r.createElement(m,s(s({ref:t},d),{},{components:n})):r.createElement(m,s({ref:t},d))}));function m(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=n.length,s=new Array(i);s[0]=c;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o[p]="string"==typeof e?e:a,s[1]=o;for(var u=2;u<i;u++)s[u]=n[u];return r.createElement.apply(null,s)}return r.createElement.apply(null,n)}c.displayName="MDXCreateElement"},93435:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>o,toc:()=>u});var r=n(87462),a=(n(67294),n(3905));const i={},s="Simple Neural Net from scratch using Autograd",o={unversionedId:"intro/autograd-model",id:"intro/autograd-model",title:"Simple Neural Net from scratch using Autograd",description:"In this section, we'll introduce the concept of automatic differentiation (autograd) by implementing a simple",source:"@site/tensor/intro/4-autograd-model.mdx",sourceDirName:"intro",slug:"/intro/autograd-model",permalink:"/tensor/intro/autograd-model",draft:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{},sidebar:"apiSidebar",previous:{title:"Basic usage",permalink:"/tensor/intro/basic-usage"},next:{title:"API",permalink:"/tensor/category/api"}},l={},u=[{value:"Create the SimpleModel class",id:"create-the-simplemodel-class",level:2},{value:"Forward pass function",id:"forward-pass-function",level:2},{value:"Backward pass function",id:"backward-pass-function",level:2},{value:"Trigger Autograd for Loss Tensor:",id:"trigger-autograd-for-loss-tensor",level:4},{value:"Update Hidden Layer Weights and Bias:",id:"update-hidden-layer-weights-and-bias",level:4},{value:"Update Output Layer Weights and Bias:",id:"update-output-layer-weights-and-bias",level:4},{value:"Training our model",id:"training-our-model",level:2},{value:"Predicting",id:"predicting",level:2},{value:"Full implementation",id:"full-implementation",level:2}],d={toc:u},p="wrapper";function h(e){let{components:t,...n}=e;return(0,a.kt)(p,(0,r.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"simple-neural-net-from-scratch-using-autograd"},"Simple Neural Net from scratch using Autograd"),(0,a.kt)("p",null,"In this section, we'll introduce the concept of automatic differentiation (autograd) by implementing a simple\nneural network model using the NumPower Autograd library. This model will showcase how NumPower Autograd's\nautograd feature can be utilized to compute gradients and update model parameters efficiently during the training process."),(0,a.kt)("p",null,"Let's walk through the implementation of a custom model using NumPower Autograd.\nThis model will have one hidden layer and will use the sigmoid activation function for the output layer.\nWe'll train the model to perform a basic binary classification task."),(0,a.kt)("h2",{id:"create-the-simplemodel-class"},"Create the SimpleModel class"),(0,a.kt)("p",null,"Let's start by creating a class that initializes the parameters we need for our neural network.\nOur neural network will have 1 hidden layer and 1 output layer."),(0,a.kt)("p",null,"For our constructor, we will need the following parameters:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"inputDim:")," Dimensionality of the input features."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"outputDim:")," Dimensionality of the output."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"hiddenSize:")," Number of neurons in the hidden layer."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("strong",{parentName:"li"},"learningRate:")," Learning rate for the optimizer.")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"<?php\nuse NDArray as nd;\nuse NumPower\\Tensor;\nuse NumPower\\NeuralNetwork\\Activations as activation;\nuse NumPower\\NeuralNetwork\\Losses as loss;\n\nclass SimpleModel\n{\n    public Tensor $weights_hidden_layer;\n    public Tensor $weights_output_layer;\n    public Tensor $hidden_bias;\n    public Tensor $output_bias;\n    private float $learningRate;\n\n    public function __construct(int $inputDim = 2,\n                                int $outputDim = 1,\n                                int $hiddenSize = 16,\n                                float $learningRate = 0.01\n    )\n    {\n        $this->learningRate = $learningRate;\n        // Initialize hidden layer weights\n        $this->weights_hidden_layer = new Tensor(\n            nd::uniform([$inputDim, $hiddenSize], -0.5, 0.5),\n            name: 'weights_hidden_layer',\n            requireGrad: True\n        );\n        // Initialize output layer weights\n        $this->weights_output_layer = new Tensor(\n            nd::uniform([$hiddenSize, $outputDim],-0.5, 0.5),\n            name: 'weights_output_layer',\n            requireGrad: True\n        );\n        // Initialize hidden layer bias\n        $this->hidden_bias = new Tensor(\n            nd::uniform([$hiddenSize],  -0.5, 0.5),\n            name: 'hidden_bias',\n            requireGrad: True\n        );\n        // Initialize output layer bias\n        $this->output_bias = new Tensor(\n            nd::uniform([$outputDim], -0.5, 0.5),\n            name: 'output_bias',\n            requireGrad: True\n        );\n    }\n}\n")),(0,a.kt)("p",null,"For each weight and bias of our layers, we will initialize a Variable using a random\nuniform algorithm from the NumPower extension."),(0,a.kt)("h2",{id:"forward-pass-function"},"Forward pass function"),(0,a.kt)("p",null,"The forward pass in a neural network is the process where the input data is passed through the network's\nlayers to produce an output. This involves several key operations, including linear transformations\n(matrix multiplications), adding biases, and applying activation functions to introduce non-linearity."),(0,a.kt)("p",null,"The forward function in the CustomModel class is responsible for computing the predictions of the neural network\nas well as the loss."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"public function forward(Tensor $x, Tensor $y): array\n{\n    // Forward pass - Hidden Layer\n    $x = $x->matmul($this->weights_hidden_layer) + $this->hidden_bias;\n    $x = activation::ReLU($x); // ReLU Activation\n\n    // Forward pass - Output Layer\n    $x = $x->matmul($this->weights_output_layer) + $this->output_bias;\n    $x = activation::sigmoid($x); // Sigmoid Activation\n\n    // Binary Cross Entropy Loss\n    $loss = loss::BinaryCrossEntropy($x, $y, name: 'loss');\n    return [$x, $loss];\n}\n")),(0,a.kt)("h2",{id:"backward-pass-function"},"Backward pass function"),(0,a.kt)("p",null,"This function, ",(0,a.kt)("inlineCode",{parentName:"p"},"backward"),", performs the backward pass in a neural network training process, which includes\nthe backpropagation of errors and the update of network weights and biases using Stochastic Gradient Descent (SGD)."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"public function backward(Tensor $loss)\n{\n    // Trigger autograd\n    $loss->backward();\n\n    // SGD (Optimizer) - Update Hidden Layer weights and bias\n    $dw_dLoss = $this->weights_hidden_layer->grad();\n\n    $this->weights_hidden_layer -= ($dw_dLoss * $this->learningRate);\n    $this->weights_hidden_layer->resetGradients();\n\n    $this->hidden_bias -= ($this->hidden_bias->grad() * $this->learningRate);\n    $this->hidden_bias->resetGradients();\n\n    // SGD (Optimizer) - Update Output Layer weights and bias\n    $db_dLoss = $this->weights_output_layer->grad();\n\n    $this->weights_output_layer -= ($db_dLoss * $this->learningRate);\n    $this->weights_output_layer->resetGradients();\n\n    $this->output_bias -= $this->output_bias - ($this->output_bias->grad() * $this->learningRate);\n    $this->output_bias->resetGradients();\n}\n")),(0,a.kt)("h4",{id:"trigger-autograd-for-loss-tensor"},"Trigger Autograd for Loss Tensor:"),(0,a.kt)("p",null,"The function starts by invoking the backward method on the loss tensor. This initiates the backpropagation process, calculating the gradients of the loss with respect to all the parameters (weights and biases) in the network."),(0,a.kt)("h4",{id:"update-hidden-layer-weights-and-bias"},"Update Hidden Layer Weights and Bias:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Retrieve Gradients:\nThe gradient of the loss with respect to the hidden layer weights is obtained using the grad() method."),(0,a.kt)("li",{parentName:"ul"},"Update Weights:\nThe hidden layer weights are updated by subtracting the product of the gradients and the learning rate from the current weights."),(0,a.kt)("li",{parentName:"ul"},"Reset Gradients:\nThe gradients for the hidden layer weights are reset to zero to prevent accumulation in the next backward pass."),(0,a.kt)("li",{parentName:"ul"},"Update Bias:\nSimilarly, the hidden layer biases are updated by subtracting the product of their gradients and the learning rate."),(0,a.kt)("li",{parentName:"ul"},"Reset Gradients:\nThe gradients for the hidden layer biases are reset.")),(0,a.kt)("h4",{id:"update-output-layer-weights-and-bias"},"Update Output Layer Weights and Bias:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Retrieve Gradients:\nThe gradient of the loss with respect to the output layer weights is obtained."),(0,a.kt)("li",{parentName:"ul"},"Update Weights:\nThe output layer weights are updated by subtracting the product of the gradients and the learning rate from the current weights."),(0,a.kt)("li",{parentName:"ul"},"Reset Gradients:\nThe gradients for the output layer weights are reset to zero."),(0,a.kt)("li",{parentName:"ul"},"Update Bias:\nThe output layer biases are updated by subtracting the product of their gradients and the learning rate."),(0,a.kt)("li",{parentName:"ul"},"Reset Gradients:\nThe gradients for the output layer biases are reset.")),(0,a.kt)("p",null,"This function ensures that the neural network parameters (weights and biases) are adjusted in response to the\nerror calculated from the predictions, which helps the model to learn and improve its accuracy over time.\nThe use of autograd simplifies the gradient computation process, while the manual updates and gradient resets\nensure the parameters are correctly adjusted for each training iteration."),(0,a.kt)("h2",{id:"training-our-model"},"Training our model"),(0,a.kt)("p",null,"This model is already sufficient to solve several binary problems. For simplicity, let's train our\nmodel to solve the XOR problem. For this, we will use 4000 epochs during training:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"$num_epochs = 4000;\n$x = new Tensor(nd::array([[0, 0], [1, 0], [1, 1], [0, 1]]), name: 'x');\n$y = new Tensor(nd::array([[0], [1], [0], [1]]), name: 'y');\n\n$model = new SimpleModel();\n\n$start = microtime(true);\nfor ($current_epoch = 0; $current_epoch < $num_epochs; $current_epoch++) {\n    // Forward Pass\n    [$prediction, $loss] = $model->forward($x, $y);\n    // Backward Pass\n    $model->backward($loss);\n    echo \"\\n Epoch ($current_epoch): \".$loss->getArray();\n}\n")),(0,a.kt)("h2",{id:"predicting"},"Predicting"),(0,a.kt)("p",null,"With our trained model, we can predict the XOR problem by performing another forward pass and checking\nthe output of this pass:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},'echo "\\nPredicted:\\n";\n$predicted = $model->forward($x, $y)[0];\necho $predicted;\n')),(0,a.kt)("p",null,"We can see that our neural network has converged and presents consistent results:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Predicted:\n[[0.102802]\n [0.876796]\n [0.0873984]\n [0.884605]]\n")),(0,a.kt)("p",null,"To use your results and tensors natively in PHP in conjunction with other libraries, you probably want to convert your\nTensor to a native PHP value. To do this, simply call the ",(0,a.kt)("inlineCode",{parentName:"p"},"toArray()")," method:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"print_r($predicted->toArray());\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"Array\n(\n    [0] => Array\n        (\n            [0] => 0.102802\n        )\n\n    [1] => Array\n        (\n            [0] => 0.876796\n        )\n\n    [2] => Array\n        (\n            [0] => 0.0873984\n        )\n\n    [3] => Array\n        (\n            [0] => 0.884605\n        )\n\n)\n")),(0,a.kt)("h2",{id:"full-implementation"},"Full implementation"),(0,a.kt)("p",null,"Finally, this is the complete implementation of our solution.\nTry updating your tensors to use the GPU using the ",(0,a.kt)("inlineCode",{parentName:"p"},"useGpu: True")," argument when creating your weights and biases."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-php"},"<?php\nrequire_once \"vendor/autoload.php\";\n\nuse NDArray as nd;\nuse NumPower\\Tensor;\nuse NumPower\\NeuralNetwork\\Activations as activation;\nuse NumPower\\NeuralNetwork\\Losses as loss;\n\nclass SimpleModel\n{\n    public Tensor $weights_hidden_layer;\n    public Tensor $weights_output_layer;\n    public Tensor $hidden_bias;\n    public Tensor $output_bias;\n    private float $learningRate;\n\n    public function __construct(int $inputDim = 2,\n                                int $outputDim = 1,\n                                int $hiddenSize = 16,\n                                float $learningRate = 0.01\n    )\n    {\n        $this->learningRate = $learningRate;\n        // Initialize hidden layer weights\n        $this->weights_hidden_layer = new Tensor(\n            nd::uniform([$inputDim, $hiddenSize], -0.5, 0.5),\n            name: 'weights_hidden_layer',\n            requireGrad: True\n        );\n        // Initialize output layer weights\n        $this->weights_output_layer = new Tensor(\n            nd::uniform([$hiddenSize, $outputDim],-0.5, 0.5),\n            name: 'weights_output_layer',\n            requireGrad: True\n        );\n        // Initialize hidden layer bias\n        $this->hidden_bias = new Tensor(\n            nd::uniform([$hiddenSize],  -0.5, 0.5),\n            name: 'hidden_bias',\n            requireGrad: True\n        );\n        // Initialize output layer bias\n        $this->output_bias = new Tensor(\n            nd::uniform([$outputDim], -0.5, 0.5),\n            name: 'output_bias',\n            requireGrad: True\n        );\n    }\n\n    public function forward(Tensor $x, Tensor $y): array\n    {\n        // Forward pass - Hidden Layer\n        $x = $x->matmul($this->weights_hidden_layer) + $this->hidden_bias;\n        $x = activation::ReLU($x); // ReLU Activation\n\n        // Forward pass - Output Layer\n        $x = $x->matmul($this->weights_output_layer) + $this->output_bias;\n        $x = activation::sigmoid($x); // Sigmoid Activation\n\n        // Mean Squared Error\n        $loss = loss::MeanSquaredError($x, $y, name: 'loss');\n        return [$x, $loss];\n    }\n\n    public function backward(Tensor $loss)\n    {\n        // Trigger autograd\n        $loss->backward();\n\n        // SGD (Optimizer) - Update Hidden Layer weights and bias\n        $dw_dLoss = $this->weights_hidden_layer->grad();\n\n        $this->weights_hidden_layer -= ($dw_dLoss * $this->learningRate);\n        $this->weights_hidden_layer->resetGradients();\n\n        $this->hidden_bias -= ($this->hidden_bias->grad() * $this->learningRate);\n        $this->hidden_bias->resetGradients();\n\n        // SGD (Optimizer) - Update Output Layer weights and bias\n        $db_dLoss = $this->weights_output_layer->grad();\n\n        $this->weights_output_layer -= ($db_dLoss * $this->learningRate);\n        $this->weights_output_layer->resetGradients();\n\n        $this->output_bias -= $this->output_bias - ($this->output_bias->grad() * $this->learningRate);\n        $this->output_bias->resetGradients();\n    }\n}\n\n$num_epochs = 4000;\n$x = new Tensor(nd::array([[0, 0], [1, 0], [1, 1], [0, 1]]), name: 'x');\n$y = new Tensor(nd::array([[0], [1], [0], [1]]), name: 'y');\n\n$model = new SimpleModel();\n\n$start = microtime(true);\nfor ($current_epoch = 0; $current_epoch < $num_epochs; $current_epoch++) {\n    // Forward Pass\n    [$prediction, $loss] = $model->forward($x, $y);\n    // Backward Pass\n    $model->backward($loss);\n    echo \"\\n Epoch ($current_epoch): \".$loss->getArray();\n}\n\necho \"\\nPredicted:\\n\";\n$predicted = $model->forward($x, $y)[0];\nprint_r($predicted->toArray());\n")))}h.isMDXComponent=!0}}]);