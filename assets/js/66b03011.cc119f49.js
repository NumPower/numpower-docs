"use strict";(self.webpackChunknumpower=self.webpackChunknumpower||[]).push([[8664],{3905:(t,e,n)=>{n.d(e,{Zo:()=>p,kt:()=>m});var a=n(67294);function i(t,e,n){return e in t?Object.defineProperty(t,e,{value:n,enumerable:!0,configurable:!0,writable:!0}):t[e]=n,t}function o(t,e){var n=Object.keys(t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(t);e&&(a=a.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),n.push.apply(n,a)}return n}function r(t){for(var e=1;e<arguments.length;e++){var n=null!=arguments[e]?arguments[e]:{};e%2?o(Object(n),!0).forEach((function(e){i(t,e,n[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(n,e))}))}return t}function s(t,e){if(null==t)return{};var n,a,i=function(t,e){if(null==t)return{};var n,a,i={},o=Object.keys(t);for(a=0;a<o.length;a++)n=o[a],e.indexOf(n)>=0||(i[n]=t[n]);return i}(t,e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(t);for(a=0;a<o.length;a++)n=o[a],e.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(t,n)&&(i[n]=t[n])}return i}var l=a.createContext({}),u=function(t){var e=a.useContext(l),n=e;return t&&(n="function"==typeof t?t(e):r(r({},e),t)),n},p=function(t){var e=u(t.components);return a.createElement(l.Provider,{value:e},t.children)},c="mdxType",h={inlineCode:"code",wrapper:function(t){var e=t.children;return a.createElement(a.Fragment,{},e)}},f=a.forwardRef((function(t,e){var n=t.components,i=t.mdxType,o=t.originalType,l=t.parentName,p=s(t,["components","mdxType","originalType","parentName"]),c=u(n),f=i,m=c["".concat(l,".").concat(f)]||c[f]||h[f]||o;return n?a.createElement(m,r(r({ref:e},p),{},{components:n})):a.createElement(m,r({ref:e},p))}));function m(t,e){var n=arguments,i=e&&e.mdxType;if("string"==typeof t||i){var o=n.length,r=new Array(o);r[0]=f;var s={};for(var l in e)hasOwnProperty.call(e,l)&&(s[l]=e[l]);s.originalType=t,s[c]="string"==typeof t?t:i,r[1]=s;for(var u=2;u<o;u++)r[u]=n[u];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}f.displayName="MDXCreateElement"},47116:(t,e,n)=>{n.r(e),n.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>u});var a=n(87462),i=(n(67294),n(3905));const o={},r="Activations",s={unversionedId:"nn/activations",id:"nn/activations",title:"Activations",description:"Activation functions are mathematical functions used in neural networks to introduce non-linearity into the model.",source:"@site/tensor/nn/1-activations.mdx",sourceDirName:"nn",slug:"/nn/activations",permalink:"/numpower-docs/tensor/nn/activations",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"apiSidebar",previous:{title:"Losses",permalink:"/numpower-docs/tensor/nn/losses"},next:{title:"Low-level Classes and Operations",permalink:"/numpower-docs/tensor/category/low-level-classes-and-operations"}},l={},u=[{value:"Non-Linear Activations",id:"non-linear-activations",level:2},{value:"tanh",id:"tanh",level:3},{value:"softplus",id:"softplus",level:3},{value:"softmax",id:"softmax",level:3},{value:"softsign",id:"softsign",level:3},{value:"ReLU",id:"relu",level:3},{value:"CELU",id:"celu",level:3},{value:"SiLU",id:"silu",level:3},{value:"SELU",id:"selu",level:3},{value:"exponential",id:"exponential",level:3},{value:"mish",id:"mish",level:3},{value:"sigmoid",id:"sigmoid",level:3},{value:"Linear Activations",id:"linear-activations",level:2},{value:"Linear",id:"linear",level:3}],p={toc:u},c="wrapper";function h(t){let{components:e,...n}=t;return(0,i.kt)(c,(0,a.Z)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"activations"},"Activations"),(0,i.kt)("p",null,"Activation functions are mathematical functions used in neural networks to introduce non-linearity into the model.\nThis non-linearity enables the network to learn complex patterns and relationships in the data. Activation\nfunctions are applied to the output of each neuron, determining whether that neuron should be activated or not."),(0,i.kt)("p",null,"Activation functions can be found through static calls in the class ",(0,i.kt)("inlineCode",{parentName:"p"},"NumPower\\NeuralNetwork\\Activations;"),"."),(0,i.kt)("h2",{id:"non-linear-activations"},"Non-Linear Activations"),(0,i.kt)("hr",null),(0,i.kt)("h3",{id:"tanh"},"tanh"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function tanh(int|float|array|\\NDArray|Tensor $x, string $name = 'out_tanh'): Tensor\n")),(0,i.kt)("p",null,"This function computes the hyperbolic tangent (tanh) activation function on the provided input.\nTanh transforms each element of the input tensor using the hyperbolic tangent function, which maps\nvalues to the range (-1, 1). It is useful for normalizing inputs in neural networks and other computational models,\nproviding a smooth transition between negative and positive values."),(0,i.kt)("h3",{id:"softplus"},"softplus"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function softplus(int|float|array|\\NDArray|Tensor $x, string $name = 'out_softplus'): Tensor\n")),(0,i.kt)("p",null,"This function computes the Softplus activation function on the provided input. Softplus transforms each element of\nthe input tensor using the natural logarithm of the sum of the exponential of the element and one. This operation\nsmooths out the output, ensuring it is always positive."),(0,i.kt)("p",null,"The resulting tensor, with the same shape as the input, represents the output of the softplus operation.\nThe optional parameter $name specifies the name of the output tensor and defaults to 'out_softplus'."),(0,i.kt)("h3",{id:"softmax"},"softmax"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function softmax(int|float|array|\\NDArray|Tensor $x,  string $name = 'out_softmax'): Tensor\n")),(0,i.kt)("p",null,"This function computes the softmax activation function on the provided input. Softmax transforms each element of\nthe input tensor into a probability distribution by exponentiating each element and then normalizing the\ntensor so that the sum of all elements equals one. This makes softmax suitable for multi-class classification tasks,\nwhere it outputs probabilities for each class."),(0,i.kt)("p",null,"The resulting tensor, with the same shape as the input, represents the output of the softmax operation.\nThe optional parameter $name specifies the name of the output tensor and defaults to 'out_softmax'."),(0,i.kt)("h3",{id:"softsign"},"softsign"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function softsign(int|float|array|\\NDArray|Tensor $x,  string $name = 'out_softsign'): Tensor\n")),(0,i.kt)("p",null,"This function computes the Softsign activation function on the provided input.\nThe softsign function transforms each element of the input tensor by dividing it by the absolute value of\nitself plus one. This transformation maps the input values to the range (-1, 1), preserving zero."),(0,i.kt)("p",null,"The resulting tensor, with the same shape as the input, represents the output of the softsign operation.\nThe optional parameter $name specifies the name of the output tensor and defaults to 'out_sigmoid'."),(0,i.kt)("h3",{id:"relu"},"ReLU"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function ReLU(int|float|array|\\NDArray|Tensor $inputs, string $name = 'out_relu'): Tensor\n")),(0,i.kt)("p",null,"This function implements the Rectified Linear Unit (ReLU) activation function for a given tensor of inputs.\nReLU activation sets all negative values in the tensor to zero, leaving positive values unchanged.\nThe function returns a tensor with the same shape as the input tensor, representing the output of the ReLU\noperation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_relu'."),(0,i.kt)("h3",{id:"celu"},"CELU"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function CELU(int|float|array|\\NDArray|Tensor $x,\n                            float $alpha = 1.0,\n                            string $name = 'out_celu'): Tensor\n")),(0,i.kt)("p",null,"This function computes the Continuous Exponential Linear Unit (CELU) activation function on the provided input.\nCELU applies a non-linear transformation that smooths negative values of the input tensor based on\nthe parameter $alpha, while leaving positive values unchanged."),(0,i.kt)("p",null,"The resulting tensor, with the same shape as the input, represents the output of the CELU operation.\nThe optional parameter $name specifies the name of the output tensor and defaults to 'out_celu'."),(0,i.kt)("h3",{id:"silu"},"SiLU"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function SiLU(int|float|array|\\NDArray|Tensor $x,\n                            float $beta = 1.0,\n                            string $name = 'out_silu'): Tensor\n")),(0,i.kt)("p",null,"This function computes the Sigmoid-weighted Linear Unit (SiLU), also known as the Swish activation function,\non the provided input. SiLU applies a non-linear transformation that smooths negative values of the input tensor\nbased on the parameter $beta, using a sigmoid function, while preserving positive values unchanged."),(0,i.kt)("p",null,"The resulting tensor, with the same shape as the input, represents the output of the SiLU operation.\nThe optional parameter $name specifies the name of the output tensor and defaults to 'out_silu'."),(0,i.kt)("h3",{id:"selu"},"SELU"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function SELU(int|float|array|\\NDArray|Tensor $inputs,\n                            float $alpha=1.67326,\n                            float $scale=1.0507,\n                            string $name = 'out_selu'): Tensor\n")),(0,i.kt)("p",null,"This function applies the Scaled Exponential Linear Unit (SELU) activation function to a tensor of inputs.\nSELU applies a scaled version of the Exponential Linear Unit (ELU), transforming each element of the\ninput tensor based on the parameters $alpha and $scale. The function returns a tensor with the same shape\nas the input tensor, representing the output of the SELU operation. The optional parameter $name specifies\nthe name of the output tensor and defaults to 'out_selu'."),(0,i.kt)("h3",{id:"exponential"},"exponential"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function exponential(int|float|array|\\NDArray|Tensor $x, string $name = 'out_exponential'): Tensor\n")),(0,i.kt)("p",null,"This function computes the exponential function on the provided input. The exponential function raises the mathematical constant\n\ud835\udc52 (approximately 2.718) to the power of each element in the input tensor. This operation is commonly used in various mathematical and statistical computations."),(0,i.kt)("p",null,"The resulting tensor, with the same shape as the input, represents the output of the exponential operation.\nThe optional parameter $name specifies the name of the output tensor and defaults to 'out_exponential'."),(0,i.kt)("h3",{id:"mish"},"mish"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function mish(int|float|array|\\NDArray|Tensor $x, string $name = 'out_mish'): Tensor\n")),(0,i.kt)("p",null,"This function computes the Mish activation function on the provided input. Mish is a relatively newer activation\nfunction that smooths and enhances the training of neural networks by introducing a non-linearity\nthat is differentiable and has favorable properties during gradient descent."),(0,i.kt)("h3",{id:"sigmoid"},"sigmoid"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function sigmoid(int|float|array|\\NDArray|Tensor $x,  string $name = 'out_sigmoid'): Tensor\n")),(0,i.kt)("p",null,"This function computes the sigmoid activation function on the provided input.\nThe sigmoid function transforms each element of the input tensor to a value between 0 and 1,\nrepresenting the probability-like output of a binary classification decision."),(0,i.kt)("p",null,"The resulting tensor, with the same shape as the input, represents the output of the sigmoid operation.\nThe optional parameter $name specifies the name of the output tensor and defaults to 'out_sigmoid'."),(0,i.kt)("h2",{id:"linear-activations"},"Linear Activations"),(0,i.kt)("hr",null),(0,i.kt)("p",null,"A linear activation function, also known as an identity activation function, is a function where the output is\ndirectly proportional to the input. In mathematical terms, if the input to the function is \ud835\udc65, the output is also \ud835\udc65."),(0,i.kt)("h3",{id:"linear"},"Linear"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-php"},"public static function linear(int|float|array|\\NDArray|Tensor $x, string $name = 'out_linear'): Tensor\n")),(0,i.kt)("p",null,"This function represents the identity or linear activation function, which simply returns the input tensor\n\ud835\udc65  unchanged. It is used when no activation function is desired or required in a specific layer of a neural\nnetwork or any other computational graph."))}h.isMDXComponent=!0}}]);