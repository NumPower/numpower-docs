[{"title":"NDArray::isGPU","type":0,"sectionRef":"#","url":"/api/devices-functions/ndarray-isgpu","content":"","keywords":""},{"title":"Return​","type":1,"pageTitle":"NDArray::isGPU","url":"/api/devices-functions/ndarray-isgpu#return","content":"Type bool True if NDArray is stored in the GPU, False otherwise. "},{"title":"NDArray::setDevice","type":0,"sectionRef":"#","url":"/api/devices-functions/ndarray-setdevice","content":"NDArray::setDevice public static function setDevice(int $deviceId): void; Specifies which GPU device to use by ID. By default, all operations are performed on GPU id = 0. Use the dumpDevices method if you want to check the ID in a multi-GPU environment.","keywords":""},{"title":"NDArray::cpu","type":0,"sectionRef":"#","url":"/api/devices-functions/ndarray-cpu","content":"","keywords":""},{"title":"Return​","type":1,"pageTitle":"NDArray::cpu","url":"/api/devices-functions/ndarray-cpu#return","content":"Type NDArray A copy of the NDArray but stored in RAM.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::cpu","url":"/api/devices-functions/ndarray-cpu#examples","content":"Example 1 use \\NDArray as nd; $a_gpu = nd::array([2, -2, 3])-&gt;gpu(); $a_cpu = $a-&gt;cpu(); $a_gpu-&gt;dump(); $a_cpu-&gt;dump(); Output ================================================= NDArray.uuid 0 NDArray.dims [ 3 ] NDArray.strides [ 4 ] NDArray.ndim 1 NDArray.device GPU NDArray.refcount 1 NDArray.descriptor.elsize 4 NDArray.descriptor.numElements 3 NDArray.descriptor.type float32 ================================================= ================================================= NDArray.uuid 1 NDArray.dims [ 3 ] NDArray.strides [ 4 ] NDArray.ndim 1 NDArray.device CPU NDArray.refcount 1 NDArray.descriptor.elsize 4 NDArray.descriptor.numElements 3 NDArray.descriptor.type float32 =================================================  "},{"title":"NDArray::gpu","type":0,"sectionRef":"#","url":"/api/devices-functions/ndarray-gpu","content":"","keywords":""},{"title":"Return​","type":1,"pageTitle":"NDArray::gpu","url":"/api/devices-functions/ndarray-gpu#return","content":"Type NDArray A copy of the NDArray but stored in VRAM.  "},{"title":"Exceptions​","type":1,"pageTitle":"NDArray::gpu","url":"/api/devices-functions/ndarray-gpu#exceptions","content":"If no devices are detected or support GPU operations, a fatal error will be raised. Fatal error: Uncaught Error: No GPU device available or CUDA not enabled in /src/test.php:8 Stack trace: #0 /src/test.php(8): NDArray-&gt;gpu()   "},{"title":"Notes​","type":1,"pageTitle":"NDArray::gpu","url":"/api/devices-functions/ndarray-gpu#notes","content":"tip CUDA DEVICES​ You can use the dumpDevices method to check which devices were detected by NumPower. Currently only video cards with CUDA support are supported (NVIDIA).  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::gpu","url":"/api/devices-functions/ndarray-gpu#examples","content":"Example 1 use \\NDArray as nd; $a = nd::array([2, -2, 3]); $a_gpu = $a-&gt;gpu(); $a-&gt;dump(); $a_gpu-&gt;dump(); Output ================================================= NDArray.uuid 0 NDArray.dims [ 3 ] NDArray.strides [ 4 ] NDArray.ndim 1 NDArray.device CPU NDArray.refcount 1 NDArray.descriptor.elsize 4 NDArray.descriptor.numElements 3 NDArray.descriptor.type float32 ================================================= ================================================= NDArray.uuid 1 NDArray.dims [ 3 ] NDArray.strides [ 4 ] NDArray.ndim 1 NDArray.device GPU NDArray.refcount 1 NDArray.descriptor.elsize 4 NDArray.descriptor.numElements 3 NDArray.descriptor.type float32 =================================================  "},{"title":"NDArray::arange","type":0,"sectionRef":"#","url":"/api/initializers/ndarray-arange","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::arange","url":"/api/initializers/ndarray-arange#parameters","content":""},{"title":"$start​","type":1,"pageTitle":"NDArray::arange","url":"/api/initializers/ndarray-arange#start","content":"Type scalar Start of the interval. Default value is 0. "},{"title":"$stop​","type":1,"pageTitle":"NDArray::arange","url":"/api/initializers/ndarray-arange#stop","content":"Type scalar Stop of the interval. "},{"title":"$step​","type":1,"pageTitle":"NDArray::arange","url":"/api/initializers/ndarray-arange#step","content":"Type scalar Step of the interval. Default value is 1.  "},{"title":"Return​","type":1,"pageTitle":"NDArray::arange","url":"/api/initializers/ndarray-arange#return","content":"Type - NDArray A single-precision (float32) NDArray with evenly spaced values.  "},{"title":"NDArray::toImage","type":0,"sectionRef":"#","url":"/api/image-support/ndarray-toimage","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::toImage","url":"/api/image-support/ndarray-toimage#notes","content":"note PHP-GD REQUIRED​ The PHP-GD extension must be installed during NumPower compilation for this function to be available. tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::toImage","url":"/api/image-support/ndarray-toimage#examples","content":"use \\NDArray as nd; $gd_image_array = nd::array(imagecreatefromjpeg(&quot;test_img.jpg&quot;)); print_r($gd_image_array-&gt;shape()); // Your operations here // ... // End of operations $gd_image = $gd_image_array-&gt;toImage(); print_r($gd_image); // Now we have a GD image imagejpeg($gd_image, &quot;out.jpg&quot;); // Save the image Array ( [0] =&gt; 3 [1] =&gt; 1200 [2] =&gt; 1920 ) GdImage Object ( )  "},{"title":"NDArray::array","type":0,"sectionRef":"#","url":"/api/initializers/ndarray-array","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::array","url":"/api/initializers/ndarray-array#parameters","content":""},{"title":"$array​","type":1,"pageTitle":"NDArray::array","url":"/api/initializers/ndarray-array#array","content":"Type array[long|double,] The PHP array to be converted to a NDArray  "},{"title":"Return​","type":1,"pageTitle":"NDArray::array","url":"/api/initializers/ndarray-array#return","content":"Type - NDArray A single-precision (float32) NDArray with the same shape and values of $array  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::array","url":"/api/initializers/ndarray-array#notes","content":"note Every floating point in PHP is a double precision (float64) so some precision may be lost during conversion.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::array","url":"/api/initializers/ndarray-array#examples","content":"Example 1 use \\NDArray as nd; $a = nd::array([[1, 2], [3, 4]]); print_r($a); [[1, 2], [3, 4]]   "},{"title":"NDArray::ones","type":0,"sectionRef":"#","url":"/api/initializers/ndarray-ones","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::ones","url":"/api/initializers/ndarray-ones#parameters","content":""},{"title":"$shape​","type":1,"pageTitle":"NDArray::ones","url":"/api/initializers/ndarray-ones#shape","content":"Type array[long,] The shape parameter can be a sequence of integers, indicating the dimensions of the array  "},{"title":"Return​","type":1,"pageTitle":"NDArray::ones","url":"/api/initializers/ndarray-ones#return","content":"Type - NDArray Return an array of shape $shape filles with ones. "},{"title":"NDArray::cholesky","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-cholesky","content":"NDArray::cholesky public static function cholesky(NDArray|array $a): NDArray; Calculates the Cholesky decomposition of a positive-definite array, decomposing it into a lower triangular matrix and its conjugate transpose.","keywords":""},{"title":"NDArray::zeros","type":0,"sectionRef":"#","url":"/api/initializers/ndarray-zeros","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::zeros","url":"/api/initializers/ndarray-zeros#parameters","content":""},{"title":"$shape​","type":1,"pageTitle":"NDArray::zeros","url":"/api/initializers/ndarray-zeros#shape","content":"Type array[long,] The shape parameter can be a sequence of integers, indicating the dimensions of the array  "},{"title":"Return​","type":1,"pageTitle":"NDArray::zeros","url":"/api/initializers/ndarray-zeros#return","content":"Type - NDArray Return an array of shape $shape filled with zeros. "},{"title":"NumPower API","type":0,"sectionRef":"#","url":"/api/intro","content":"","keywords":""},{"title":"How This API Documentation is Organized​","type":1,"pageTitle":"NumPower API","url":"/api/intro#how-this-api-documentation-is-organized","content":"This API documentation is divided into various categories to help you find the information you need effectively: Math: This section encompasses all mathematical functions provided by NumPower, including arithmetic and trigonometric operations. Initializers: Learn about the multiple ways to initialize arrays in NumPower, with functions to create arrays from existing data or initialize arrays with preset values. Linear Algebra: Dive into the functions associated with linear algebra operations, such as matrix multiplication, determinants, solving linear equations, among others. Logic Functions: This category includes functions for evaluating logical statements, including logical AND, OR, NOT operations, and comparison operations. Manipulation: Understand more about the functions that allow you to alter the shape, size, and structure of NumPower arrays. Random: Explore the functions provided by NumPower for generating arrays of random numbers, drawn from various probability distributions. Statistics: This part houses statistical functions for operations such as averages, medians and variances. Low-level Debug: This section includes functions and methods to assist in debugging. Whether you're a beginner embarking on your journey in numerical computing, a developer eager to incorporate numerical computing into your AI projects, or an ML enthusiast keen on leveraging PHP, this documentation will help you navigate NumPower effectively. As the NumPower community grows and the library evolves, we remain committed to keeping this documentation comprehensive, up-to-date, and user-friendly. "},{"title":"NDArray::det","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-det","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::det","url":"/api/linear-algebra/ndarray-det#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::cond","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-cond","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::cond","url":"/api/linear-algebra/ndarray-cond#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::dot","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-dot","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::dot","url":"/api/linear-algebra/ndarray-dot#parameters","content":""},{"title":"$a $b​","type":1,"pageTitle":"NDArray::dot","url":"/api/linear-algebra/ndarray-dot#a-b","content":"Type NDArray|array|long|double The arrays to perfom the dot product.  "},{"title":"Return​","type":1,"pageTitle":"NDArray::dot","url":"/api/linear-algebra/ndarray-dot#return","content":"Type - NDArray If both $a and $b are scalars or 1-D arrays, the dot product operation yields a scalar value. In this case, a single scalar is returned as the result.If either $a or $b is a scalar and the other is an array, or if both $a and $b are arrays with any dimensionality greater than 1, the dot product operation results in an array. The returned array will have a shape determined by the dimensions of $a and $b according to the dot product rules.  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::dot","url":"/api/linear-algebra/ndarray-dot#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::dot","url":"/api/linear-algebra/ndarray-dot#examples","content":"Example 1Example 2Example 3 use \\NDArray as nd; $a = nd::array([[1, 2],[3, 4]]); $b = nd::array([1, 2]); $result = nd::dot($a, $b); print_r($result); [5, 11]  "},{"title":"NDArray::identity","type":0,"sectionRef":"#","url":"/api/initializers/ndarray-identity","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::identity","url":"/api/initializers/ndarray-identity#parameters","content":""},{"title":"$size​","type":1,"pageTitle":"NDArray::identity","url":"/api/initializers/ndarray-identity#size","content":"Type long Number of rows and columns of the new square array of size ($size, $size)  "},{"title":"Return​","type":1,"pageTitle":"NDArray::identity","url":"/api/initializers/ndarray-identity#return","content":"Type - NDArray Return a new square array of size ($size, $size)  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::identity","url":"/api/initializers/ndarray-identity#examples","content":"Example 1Example 2Example 3 use \\NDArray as nd; $a = nd::identity(10); print_r($a); [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 1, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]   "},{"title":"Exceptions​","type":1,"pageTitle":"NDArray::identity","url":"/api/initializers/ndarray-identity#exceptions","content":"If $size is less than 0 a Fatal error will be raised Fatal error: Uncaught Error: negative dimensions are not allowed in /src/test.php:4  "},{"title":"NDArray::eig","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-eig","content":"NDArray::eig public static function eig(NDArray|array $a): array; Computes the eigenvalues and eigenvectors of a square array.","keywords":""},{"title":"NDArray::inv","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-inv","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::inv","url":"/api/linear-algebra/ndarray-inv#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::inner","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-inner","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::inner","url":"/api/linear-algebra/ndarray-inner#parameters","content":""},{"title":"$a $b​","type":1,"pageTitle":"NDArray::inner","url":"/api/linear-algebra/ndarray-inner#a-b","content":"Type NDArray|array|long|double The arrays to perfom the inner product.  "},{"title":"Return​","type":1,"pageTitle":"NDArray::inner","url":"/api/linear-algebra/ndarray-inner#return","content":"Type NDArray If both $a and $b are scalars or 1-D arrays, the function will return a scalar value. Otherwise, if the input arrays have more than one dimension, an array will be returned.  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::inner","url":"/api/linear-algebra/ndarray-inner#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::inner","url":"/api/linear-algebra/ndarray-inner#examples","content":"Example 1Example 2 use \\NDArray as nd; $a = nd::array([2, -2, 3]); $b = nd::array([1, -1.5, 3]); $result = nd::inner($a, $b); print_r($result); 14  "},{"title":"NDArray::lu","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-lu","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::lu","url":"/api/linear-algebra/ndarray-lu#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::norm","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-norm","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::norm","url":"/api/linear-algebra/ndarray-norm#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::matmul","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-matmul","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::matmul","url":"/api/linear-algebra/ndarray-matmul#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::lstsq","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-lstsq","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::lstsq","url":"/api/linear-algebra/ndarray-lstsq#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::matrix_rank","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-matrixrank","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::matrix_rank","url":"/api/linear-algebra/ndarray-matrixrank#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::solve","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-solve","content":"NDArray::solve public static function solve(NDArray|array $a, NDArray|array $b): NDArray; Solves a linear system of equations for x, where Ax = b, and A and b are given arrays.","keywords":""},{"title":"NDArray::svd","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-svd","content":"","keywords":""},{"title":"Return​","type":1,"pageTitle":"NDArray::svd","url":"/api/linear-algebra/ndarray-svd#return","content":"Type - array[NDArray, NDArray, NDArray] PHP array containing the Unitary Arrays (U) [0], the vector(s) with the singular values (S) [1] and the unitary arrays (Vh) [2]  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::svd","url":"/api/linear-algebra/ndarray-svd#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::outer","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-outer","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::outer","url":"/api/linear-algebra/ndarray-outer#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::allclose","type":0,"sectionRef":"#","url":"/api/logic/ndarray-allclose","content":"NDArray::allclose public static function allclose(NDArray|array|float|int $a, NDArray|array|float|int $b, float $rtol = 1e-05, float $atol = 1e-08): NDArray; Checks if all elements in two arrays are approximately equal within a specified tolerance element-wise.","keywords":""},{"title":"NDArray::qr","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-qr","content":"NDArray::qr public static function qr(NDArray|array $a): array; Calculates the QR decomposition of an array, which expresses it as the product of an orthogonal matrix (Q) and an upper triangular matrix (R).","keywords":""},{"title":"NDArray::trace","type":0,"sectionRef":"#","url":"/api/linear-algebra/ndarray-trace","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::trace","url":"/api/linear-algebra/ndarray-trace#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::equal","type":0,"sectionRef":"#","url":"/api/logic/ndarray-equal","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::equal","url":"/api/logic/ndarray-equal#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::greater","type":0,"sectionRef":"#","url":"/api/logic/ndarray-greater","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::greater","url":"/api/logic/ndarray-greater#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::greater_equal","type":0,"sectionRef":"#","url":"/api/logic/ndarray-greater_equal","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::greater_equal","url":"/api/logic/ndarray-greater_equal#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::all","type":0,"sectionRef":"#","url":"/api/logic/ndarray-all","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::all","url":"/api/logic/ndarray-all#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::less","type":0,"sectionRef":"#","url":"/api/logic/ndarray-less","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::less","url":"/api/logic/ndarray-less#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::not_equal","type":0,"sectionRef":"#","url":"/api/logic/ndarray-not_equal","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::not_equal","url":"/api/logic/ndarray-not_equal#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::dump","type":0,"sectionRef":"#","url":"/api/low-level-debug/ndarray-dump","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"NDArray::dump","url":"/api/low-level-debug/ndarray-dump#examples","content":"Example 1 use \\NDArray as nd; $a = nd::array([[1, 2, 3, 4]]); $a-&gt;dump(); Output ================================================= NDArray.uuid 0 NDArray.dims [ 1 4 ] NDArray.strides [ 16 4 ] NDArray.ndim 2 NDArray.device CPU NDArray.refcount 1 NDArray.descriptor.elsize 4 NDArray.descriptor.numElements 4 NDArray.descriptor.type float32 =================================================  "},{"title":"NDArray::atleast_1d","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-atleast_1d","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::atleast_1d","url":"/api/manipulation/ndarray-atleast_1d#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM). "},{"title":"NDArray::dumpDevices","type":0,"sectionRef":"#","url":"/api/low-level-debug/ndarray-dumpDevices","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"NDArray::dumpDevices","url":"/api/low-level-debug/ndarray-dumpDevices#examples","content":"Example 1 use \\NDArray as nd; nd::dumpDevices(); Output ============================================================================== Number of CUDA devices: 1 --------------------------------------------------------------------------- Device 0: NVIDIA GeForce RTX 2070 SUPER Compute capability: 7.5 Total global memory: 8358854656 bytes Max threads per block: 1024 Max threads in X-dimension of block: 1024 Max threads in Y-dimension of block: 1024 Max threads in Z-dimension of block: 64 Max grid size in X-dimension: 2147483647 Max grid size in Y-dimension: 65535 Max grid size in Z-dimension: 65535 Max grid size in Z-dimension: 65535 Max grid size in Z-dimension: 65535 Max grid size in Z-dimension: 65535 --------------------------------------------------------------------------- ==============================================================================  "},{"title":"NDArray::less_equal","type":0,"sectionRef":"#","url":"/api/logic/ndarray-less_equal","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::less_equal","url":"/api/logic/ndarray-less_equal#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::atleast_2d","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-atleast_2d","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::atleast_2d","url":"/api/manipulation/ndarray-atleast_2d#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM). "},{"title":"NDArray::atleast_3d","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-atleast_3d","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::atleast_3d","url":"/api/manipulation/ndarray-atleast_3d#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM). "},{"title":"NDArray::copy","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-copy","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::copy","url":"/api/manipulation/ndarray-copy#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::flatten","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-flat","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::flatten","url":"/api/manipulation/ndarray-flat#parameters","content":""},{"title":"$a​","type":1,"pageTitle":"NDArray::flatten","url":"/api/manipulation/ndarray-flat#a","content":"Type NDArray array scalar Target array  "},{"title":"Return​","type":1,"pageTitle":"NDArray::flatten","url":"/api/manipulation/ndarray-flat#return","content":"Type - NDArray A copy of $a, with dimensions collapsed to 1-d, in the same device.  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::flatten","url":"/api/manipulation/ndarray-flat#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::shape","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-shape","content":"NDArray::shape public function shape(): array; Return a PHP array representing the shape of the NDArray","keywords":""},{"title":"NDArray::size","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-size","content":"NDArray::size public function size(): int; Return the total number of elements in the NDArray.","keywords":""},{"title":"NDArray::reshape","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-reshape","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::reshape","url":"/api/manipulation/ndarray-reshape#parameters","content":""},{"title":"$shape​","type":1,"pageTitle":"NDArray::reshape","url":"/api/manipulation/ndarray-reshape#shape","content":"Type array The new shape of the NDArray.  "},{"title":"Return​","type":1,"pageTitle":"NDArray::reshape","url":"/api/manipulation/ndarray-reshape#return","content":"Type - NDArray Return a view of the array with shape $shape. To be compatible, the new shape must have the same amount of elements as the old one. "},{"title":"NDArray::expand_dims","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-expand_dims","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::expand_dims","url":"/api/manipulation/ndarray-expand_dims#parameters","content":""},{"title":"$target​","type":1,"pageTitle":"NDArray::expand_dims","url":"/api/manipulation/ndarray-expand_dims#target","content":"Type - NDArray | array | GdImageTarget array. "},{"title":"$axis​","type":1,"pageTitle":"NDArray::expand_dims","url":"/api/manipulation/ndarray-expand_dims#axis","content":"Type - array | intThis parameter specifies the position where the new axis (or axes) will be inserted within the expanded array.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::expand_dims","url":"/api/manipulation/ndarray-expand_dims#examples","content":"Example 1Example 2Example 3 use \\NDArray as nd; $a = nd::array([[1, 2], [3, 4]]); echo nd::expand_dims($a, [0, 1]); Output [[[1, 2, 3, 4]]]   "},{"title":"Notes​","type":1,"pageTitle":"NDArray::expand_dims","url":"/api/manipulation/ndarray-expand_dims#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM). "},{"title":"NDArray::toArray","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-toArray","content":"NDArray::toArray public function toArray(): array; Return a PHP array with the same shape and a copy of values of the NDArray.","keywords":""},{"title":"NDArray::transpose","type":0,"sectionRef":"#","url":"/api/manipulation/ndarray-transpose","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::transpose","url":"/api/manipulation/ndarray-transpose#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::mod","type":0,"sectionRef":"#","url":"/api/mathematical-functions/arithmetic/ndarray-mod","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::mod","url":"/api/mathematical-functions/arithmetic/ndarray-mod#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::divide","type":0,"sectionRef":"#","url":"/api/mathematical-functions/arithmetic/ndarray-divide","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::divide","url":"/api/mathematical-functions/arithmetic/ndarray-divide#parameters","content":""},{"title":"$a $b​","type":1,"pageTitle":"NDArray::divide","url":"/api/mathematical-functions/arithmetic/ndarray-divide#a-b","content":"Type - NDArray | array | scalarInput arrays  "},{"title":"Return​","type":1,"pageTitle":"NDArray::divide","url":"/api/mathematical-functions/arithmetic/ndarray-divide#return","content":""},{"title":"NDArray​","type":1,"pageTitle":"NDArray::divide","url":"/api/mathematical-functions/arithmetic/ndarray-divide#ndarray","content":"Array with the division between $a and $b element-wise  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::divide","url":"/api/mathematical-functions/arithmetic/ndarray-divide#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::divide","url":"/api/mathematical-functions/arithmetic/ndarray-divide#examples","content":"Example 1Example 2Example 3 use \\NDArray as nd; $c = nd::divide([5, 2, -3], [4, 3, 2]); print_r($c); [1.25, 0.666667, -1.5]  "},{"title":"NDArray::add","type":0,"sectionRef":"#","url":"/api/mathematical-functions/arithmetic/ndarray-add","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::add","url":"/api/mathematical-functions/arithmetic/ndarray-add#parameters","content":"$a $b​ Type - NDArray | array | scalarThe arrays to be added, $a and $b must be of the same shape.  "},{"title":"Return​","type":1,"pageTitle":"NDArray::add","url":"/api/mathematical-functions/arithmetic/ndarray-add#return","content":"NDArray​ The sum of $a and $b  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::add","url":"/api/mathematical-functions/arithmetic/ndarray-add#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::add","url":"/api/mathematical-functions/arithmetic/ndarray-add#examples","content":"Example 1Example 2Example 3 use \\NDArray as nd; $a = new nd([[2, -2], [1, -1]]); $b = new nd([[3, -3], [2, -1]]); $c = $a + $b; print_r($c); [[5, -5], [3, -2]]  "},{"title":"NDArray::pow","type":0,"sectionRef":"#","url":"/api/mathematical-functions/arithmetic/ndarray-pow","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::pow","url":"/api/mathematical-functions/arithmetic/ndarray-pow#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::negative","type":0,"sectionRef":"#","url":"/api/mathematical-functions/arithmetic/ndarray-negative","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::negative","url":"/api/mathematical-functions/arithmetic/ndarray-negative#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::multiply","type":0,"sectionRef":"#","url":"/api/mathematical-functions/arithmetic/ndarray-multiply","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::multiply","url":"/api/mathematical-functions/arithmetic/ndarray-multiply#parameters","content":"$a $b​ Type - NDArray | array | scalarThe arrays to be multiplied.  "},{"title":"Return​","type":1,"pageTitle":"NDArray::multiply","url":"/api/mathematical-functions/arithmetic/ndarray-multiply#return","content":"NDArray​ The multiplication of $a and $b element-wise  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::multiply","url":"/api/mathematical-functions/arithmetic/ndarray-multiply#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::multiply","url":"/api/mathematical-functions/arithmetic/ndarray-multiply#examples","content":"Example 1Example 2Example 3 use \\NDArray as nd; $a = new nd([[2, -2], [1, -1]]); $b = new nd([[3, -3], [2, -1]]); $c = $a * $b; print_r($c); [[6, 6], [2, 1]]  "},{"title":"NDArray::exp2","type":0,"sectionRef":"#","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-exp2","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::exp2","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-exp2#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::exp","type":0,"sectionRef":"#","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-exp","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::exp","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-exp#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::log10","type":0,"sectionRef":"#","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-log10","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::log10","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-log10#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::subtract","type":0,"sectionRef":"#","url":"/api/mathematical-functions/arithmetic/ndarray-subtract","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::subtract","url":"/api/mathematical-functions/arithmetic/ndarray-subtract#parameters","content":""},{"title":"$a $b​","type":1,"pageTitle":"NDArray::subtract","url":"/api/mathematical-functions/arithmetic/ndarray-subtract#a-b","content":"Type - NDArray | array | scalarInput arrays  "},{"title":"Return​","type":1,"pageTitle":"NDArray::subtract","url":"/api/mathematical-functions/arithmetic/ndarray-subtract#return","content":""},{"title":"NDArray​","type":1,"pageTitle":"NDArray::subtract","url":"/api/mathematical-functions/arithmetic/ndarray-subtract#ndarray","content":"Element-wise subtraction of $a and $b element-wise  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::subtract","url":"/api/mathematical-functions/arithmetic/ndarray-subtract#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::subtract","url":"/api/mathematical-functions/arithmetic/ndarray-subtract#examples","content":"Example 1Example 2Example 3 use \\NDArray as nd; $a = new nd([[1, 2], [3, 4]]); $b = new nd([[1, 2], [3, 4]]); $c = nd::subtract($a, $b); print_r($c); [[0, 0], [0, 0]]  "},{"title":"NDArray::expm1","type":0,"sectionRef":"#","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-expm1","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::expm1","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-expm1#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::log","type":0,"sectionRef":"#","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-log","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::log","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-log#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::logb","type":0,"sectionRef":"#","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-logb","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::logb","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-logb#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::log1p","type":0,"sectionRef":"#","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-log1p","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::log1p","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-log1p#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::log2","type":0,"sectionRef":"#","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-log2","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::log2","url":"/api/mathematical-functions/exponents-and-logarithms/ndarray-log2#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::max","type":0,"sectionRef":"#","url":"/api/mathematical-functions/extrema-finding/ndarray-max","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::max","url":"/api/mathematical-functions/extrema-finding/ndarray-max#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::min","type":0,"sectionRef":"#","url":"/api/mathematical-functions/extrema-finding/ndarray-min","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::min","url":"/api/mathematical-functions/extrema-finding/ndarray-min#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::arcsinh","type":0,"sectionRef":"#","url":"/api/mathematical-functions/hyperbolic/ndarray-arcsinh","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::arcsinh","url":"/api/mathematical-functions/hyperbolic/ndarray-arcsinh#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::arctanh","type":0,"sectionRef":"#","url":"/api/mathematical-functions/hyperbolic/ndarray-arctanh","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::arctanh","url":"/api/mathematical-functions/hyperbolic/ndarray-arctanh#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::arccosh","type":0,"sectionRef":"#","url":"/api/mathematical-functions/hyperbolic/ndarray-arccosh","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::arccosh","url":"/api/mathematical-functions/hyperbolic/ndarray-arccosh#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::tanh","type":0,"sectionRef":"#","url":"/api/mathematical-functions/hyperbolic/ndarray-tanh","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::tanh","url":"/api/mathematical-functions/hyperbolic/ndarray-tanh#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::cosh","type":0,"sectionRef":"#","url":"/api/mathematical-functions/hyperbolic/ndarray-cosh","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::cosh","url":"/api/mathematical-functions/hyperbolic/ndarray-cosh#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::sinh","type":0,"sectionRef":"#","url":"/api/mathematical-functions/hyperbolic/ndarray-sinh","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::sinh","url":"/api/mathematical-functions/hyperbolic/ndarray-sinh#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::sign","type":0,"sectionRef":"#","url":"/api/mathematical-functions/miscellaneous/ndarray-sign","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::sign","url":"/api/mathematical-functions/miscellaneous/ndarray-sign#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::abs","type":0,"sectionRef":"#","url":"/api/mathematical-functions/miscellaneous/ndarray-abs","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::abs","url":"/api/mathematical-functions/miscellaneous/ndarray-abs#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::clip","type":0,"sectionRef":"#","url":"/api/mathematical-functions/miscellaneous/ndarray-clip","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::clip","url":"/api/mathematical-functions/miscellaneous/ndarray-clip#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::sinc","type":0,"sectionRef":"#","url":"/api/mathematical-functions/miscellaneous/ndarray-sinc","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::sinc","url":"/api/mathematical-functions/miscellaneous/ndarray-sinc#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::ceil","type":0,"sectionRef":"#","url":"/api/mathematical-functions/rounding/ndarray-ceil","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::ceil","url":"/api/mathematical-functions/rounding/ndarray-ceil#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::sqrt","type":0,"sectionRef":"#","url":"/api/mathematical-functions/miscellaneous/ndarray-sqrt","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::sqrt","url":"/api/mathematical-functions/miscellaneous/ndarray-sqrt#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::square","type":0,"sectionRef":"#","url":"/api/mathematical-functions/miscellaneous/ndarray-square","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::square","url":"/api/mathematical-functions/miscellaneous/ndarray-square#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::fix","type":0,"sectionRef":"#","url":"/api/mathematical-functions/rounding/ndarray-fix","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::fix","url":"/api/mathematical-functions/rounding/ndarray-fix#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::floor","type":0,"sectionRef":"#","url":"/api/mathematical-functions/rounding/ndarray-floor","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::floor","url":"/api/mathematical-functions/rounding/ndarray-floor#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::rint","type":0,"sectionRef":"#","url":"/api/mathematical-functions/rounding/ndarray-rint","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::rint","url":"/api/mathematical-functions/rounding/ndarray-rint#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::round","type":0,"sectionRef":"#","url":"/api/mathematical-functions/rounding/ndarray-round","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::round","url":"/api/mathematical-functions/rounding/ndarray-round#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::trunc","type":0,"sectionRef":"#","url":"/api/mathematical-functions/rounding/ndarray-trunc","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::trunc","url":"/api/mathematical-functions/rounding/ndarray-trunc#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::sum","type":0,"sectionRef":"#","url":"/api/mathematical-functions/sum-products-differences/ndarray-sum","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::sum","url":"/api/mathematical-functions/sum-products-differences/ndarray-sum#parameters","content":""},{"title":"$a​","type":1,"pageTitle":"NDArray::sum","url":"/api/mathematical-functions/sum-products-differences/ndarray-sum#a","content":"Type NDArray|array The input array. "},{"title":"$axis​","type":1,"pageTitle":"NDArray::sum","url":"/api/mathematical-functions/sum-products-differences/ndarray-sum#axis","content":"Type long Specifies the axis along which the sum is performed. By default, (axis=NULL), the function sums all elements of the input array.  "},{"title":"Return​","type":1,"pageTitle":"NDArray::sum","url":"/api/mathematical-functions/sum-products-differences/ndarray-sum#return","content":"Type - NDArray double The function returns the summed array along the specified axis, resulting in an array with the same shape as the input array, but with the specified axis removed. If the input array is 0-dimensional or if axis=NULL, a scalar value is returned.  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::sum","url":"/api/mathematical-functions/sum-products-differences/ndarray-sum#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::arccos","type":0,"sectionRef":"#","url":"/api/mathematical-functions/trigonometric/ndarray-arccos","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::arccos","url":"/api/mathematical-functions/trigonometric/ndarray-arccos#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::arcsin","type":0,"sectionRef":"#","url":"/api/mathematical-functions/trigonometric/ndarray-arcsin","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::arcsin","url":"/api/mathematical-functions/trigonometric/ndarray-arcsin#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::prod","type":0,"sectionRef":"#","url":"/api/mathematical-functions/sum-products-differences/ndarray-prod","content":"","keywords":""},{"title":"The default, axis=NULL, will calculate the product of all the elements in the input array.​","type":1,"pageTitle":"NDArray::prod","url":"/api/mathematical-functions/sum-products-differences/ndarray-prod#the-default-axisnull-will-calculate-the-product-of-all-the-elements-in-the-input-array","content":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::prod","url":"/api/mathematical-functions/sum-products-differences/ndarray-prod#parameters","content":""},{"title":"$a​","type":1,"pageTitle":"NDArray::prod","url":"/api/mathematical-functions/sum-products-differences/ndarray-prod#a","content":"Type - NDArray|array|scalarInput array "},{"title":"$axis​","type":1,"pageTitle":"NDArray::prod","url":"/api/mathematical-functions/sum-products-differences/ndarray-prod#axis","content":"Type - NDArray|array|scalarThe axis to perform the product. If $axis is NULL, will calculate the product of all the elements of $a.  "},{"title":"Return​","type":1,"pageTitle":"NDArray::prod","url":"/api/mathematical-functions/sum-products-differences/ndarray-prod#return","content":""},{"title":"NDArray​","type":1,"pageTitle":"NDArray::prod","url":"/api/mathematical-functions/sum-products-differences/ndarray-prod#ndarray","content":"The product of $a. If $axis is not NULL, the specified axis is removed.  "},{"title":"Examples​","type":1,"pageTitle":"NDArray::prod","url":"/api/mathematical-functions/sum-products-differences/ndarray-prod#examples","content":"Example 1Example 2Example 3 use \\NDArray as nd; $a = new nd([[1, 2], [3, 4]]); $c = nd::prod($a); print_r($c); 24  "},{"title":"NDArray::arctan","type":0,"sectionRef":"#","url":"/api/mathematical-functions/trigonometric/ndarray-arctan","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::arctan","url":"/api/mathematical-functions/trigonometric/ndarray-arctan#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::degrees","type":0,"sectionRef":"#","url":"/api/mathematical-functions/trigonometric/ndarray-degrees","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::degrees","url":"/api/mathematical-functions/trigonometric/ndarray-degrees#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::cos","type":0,"sectionRef":"#","url":"/api/mathematical-functions/trigonometric/ndarray-cos","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::cos","url":"/api/mathematical-functions/trigonometric/ndarray-cos#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::radians","type":0,"sectionRef":"#","url":"/api/mathematical-functions/trigonometric/ndarray-radians","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::radians","url":"/api/mathematical-functions/trigonometric/ndarray-radians#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::sin","type":0,"sectionRef":"#","url":"/api/mathematical-functions/trigonometric/ndarray-sin","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::sin","url":"/api/mathematical-functions/trigonometric/ndarray-sin#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::normal","type":0,"sectionRef":"#","url":"/api/random/ndarray-normal","content":"NDArray::normal public static function normal(array $size, float $loc = 0.0, float $scale = 1.0): NDArray; Generates an array of random numbers from a normal distribution. The normal distribution, also known as the Gaussian distribution, is a continuous probability distribution that is symmetric and bell-shaped.","keywords":""},{"title":"NDArray::tan","type":0,"sectionRef":"#","url":"/api/mathematical-functions/trigonometric/ndarray-tan","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::tan","url":"/api/mathematical-functions/trigonometric/ndarray-tan#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::poisson","type":0,"sectionRef":"#","url":"/api/random/ndarray-poisson","content":"NDArray::poisson public static function poisson(array $size, float $lam = 1.0): NDArray; Generates an array of random integers from a Poisson distribution. The Poisson distribution models the number of events occurring in fixed intervals of time or space, given the average rate of occurrence.","keywords":""},{"title":"NDArray::standard_normal","type":0,"sectionRef":"#","url":"/api/random/ndarray-standard_normal","content":"NDArray::standard_normal public static function standard_normal(array $size): NDArray; Generates an array of random numbers from the standard normal distribution. The standard normal distribution is a special case of the normal distribution with mean (μ) equal to 0 and standard deviation (σ) equal to 1.","keywords":""},{"title":"NDArray::uniform","type":0,"sectionRef":"#","url":"/api/random/ndarray-uniform","content":"NDArray::uniform public static function uniform(array $size, float $low = 0.0, float $high = 1.0): NDArray; Generates an array of random numbers from a uniform distribution. The uniform distribution provides an equal probability for each value within a specified range.","keywords":""},{"title":"NDArray::average","type":0,"sectionRef":"#","url":"/api/statistics/ndarray-average","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::average","url":"/api/statistics/ndarray-average#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::mean","type":0,"sectionRef":"#","url":"/api/statistics/ndarray-mean","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::mean","url":"/api/statistics/ndarray-mean#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::median","type":0,"sectionRef":"#","url":"/api/statistics/ndarray-median","content":"NDArray::median public static function median(NDArray|array|float|int $a): float|int; The median of the elements in the array. It sorts the array, and if the number of elements is odd, it returns the middle value; if the number of elements is even, it returns the average of the two middle values","keywords":""},{"title":"NDArray::convolve2d","type":0,"sectionRef":"#","url":"/api/signal-processing/ndarray-convolve2d","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"NDArray::convolve2d","url":"/api/signal-processing/ndarray-convolve2d#parameters","content":""},{"title":"$a $b​","type":1,"pageTitle":"NDArray::convolve2d","url":"/api/signal-processing/ndarray-convolve2d#a-b","content":"Type - NDArray | array | GdImageThe arrays to perform the convolution. "},{"title":"$mode​","type":1,"pageTitle":"NDArray::convolve2d","url":"/api/signal-processing/ndarray-convolve2d#mode","content":"Type - stringThe size of the output. Can be: full, valid and same "},{"title":"$boundary​","type":1,"pageTitle":"NDArray::convolve2d","url":"/api/signal-processing/ndarray-convolve2d#boundary","content":"Type - stringA flag indicating how to handle boundaries. Can be: fill, wrap and symm  "},{"title":"Notes​","type":1,"pageTitle":"NDArray::convolve2d","url":"/api/signal-processing/ndarray-convolve2d#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"NDArray::quantile","type":0,"sectionRef":"#","url":"/api/statistics/ndarray-quantile","content":"NDArray::quantile ublic static function quantile(NDArray|array|float|int $a, float|int $q): float|int; Computes the specified quantile of the elements in the array. A quantile represents a particular value below which a given percentage of data falls. For example, the median is the 50th quantile.","keywords":""},{"title":"NDArray::std","type":0,"sectionRef":"#","url":"/api/statistics/ndarray-std","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::std","url":"/api/statistics/ndarray-std#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"Image Processing","type":0,"sectionRef":"#","url":"/docs/image-processing","content":"Image Processing When NumPower is compiled in an environment with PHP-GD available, functions to facilitate image manipulation will be available for use. You can use a GdImage image to build an NDArray or as an argument for the different types of operations available, making image processing fast and with the possibility of performing image processing using a GPU. use \\NDArray as nd; $gd_image = imagecreatefromjpeg(&quot;test_img.jpg&quot;); $gd_image_array = nd:array($gd_image); ","keywords":""},{"title":"NDArray::variance","type":0,"sectionRef":"#","url":"/api/statistics/ndarray-variance","content":"","keywords":""},{"title":"Notes​","type":1,"pageTitle":"NDArray::variance","url":"/api/statistics/ndarray-variance#notes","content":"tip GPU SUPPORTED​ This operation is supported by GPU (VRAM) and contains a custom CUDA kernel. "},{"title":"About NumPower","type":0,"sectionRef":"#","url":"/docs/intro","content":"About NumPower Inspired by NumPy, the NumPower library was created to provide the foundation for efficient scientific computing in PHP, as well as leverage the machine learning tools and libraries that already exist and can benefit from it. This C extension developed for PHP can be used to considerably speed up mathematical operations on large datasets and facilitate the manipulation, creation and operation of N-dimensional tensors.","keywords":""},{"title":"Iterating","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/iterating-ndarray","content":"Iterating NDArray does implement the Traversable interface. You can iterate over an NDArray as you would a PHP array. &lt;?php $a = new NDArray([[1, 2], [3, 4]]); foreach($a as $row) { echo(&quot;\\nRow\\n&quot;); print_r($row); } Output Row [1, 2] Row [3, 4] When you iterate, slice or reshape an NDArray, NumPower will share the same data buffer as the original array, avoiding copies and memory I/O, reducing RAM consumption. In some cases a copy may be made automatically. note Iterating over NDArray is not faster than iterating over a PHP array. Mathematical operations with NDArrays are faster when using one of the methods available in NumPower.","keywords":""},{"title":"GPU Support","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/gpu","content":"","keywords":""},{"title":"Copy NDArray to GPU​","type":1,"pageTitle":"GPU Support","url":"/docs/tutorial-basics/gpu#copy-ndarray-to-gpu","content":"To copy an NDArray to your video card, just use the gpu() method. This method will return a new NDArray with the data copied to your VRAM. &lt;?php use \\NDArray as nd; $a = nd::ones([10, 10]); $a_gpu = $a-&gt;gpu();  In this example we create an NDArray with format (10, 10) full of one. Then we use the gpu() method to create a copy of this NDArray on the GPU. "},{"title":"Copy NDArray to CPU​","type":1,"pageTitle":"GPU Support","url":"/docs/tutorial-basics/gpu#copy-ndarray-to-cpu","content":"In most cases, you will want your NDArray stored in your RAM. To copy an NDArray that is stored in your VRAM (GPU) to your RAM (CPU), just use the cpu() method. &lt;?php use \\NDArray as nd; $a = nd::ones([10, 10]); $a_gpu = $a-&gt;gpu(); // OPERATIONS $result = $a_gpu-&gt;cpu();  "},{"title":"GPU and CPU operations​","type":1,"pageTitle":"GPU Support","url":"/docs/tutorial-basics/gpu#gpu-and-cpu-operations","content":"In operations involving more than one tensor, like NDArray::add, both tensors involved must be on the same device. Operations between arrays on different devices will raise an exception. In this first version of NumPower, we want the user to explicitly say where they want to store their data, so automatic copies between GPU and CPU are not available. danger Some GPU-incompatible operations may raise an exception. In these cases the user must copy the tensor to the CPU manually using the appropriate method. "},{"title":"The NDArray Object","type":0,"sectionRef":"#","url":"/docs/ndarray","content":"","keywords":""},{"title":"Converting back to a PHP array​","type":1,"pageTitle":"The NDArray Object","url":"/docs/ndarray#converting-back-to-a-php-array","content":"To use your NDArray in other PHP libraries, you can convert your NDArrays to PHP arrays by calling the toArray method: &lt;?php $a = new NDArray([[1, 2], [3, 4]]); print_r($a-&gt;toArray());  Output Array ( [0] =&gt; Array ( [0] =&gt; 1 [1] =&gt; 2 ) [1] =&gt; Array ( [0] =&gt; 3 [1] =&gt; 4 ) )  "},{"title":"Single Precision Float​","type":1,"pageTitle":"The NDArray Object","url":"/docs/ndarray#single-precision-float","content":"Unlike the PHP runtime, which uses double precision, NDArrays use single precision floats by default. This significantly increases the speed of some operations and reduces RAM consumption by up to 2 times compared to an array using PHP arrays. On the other hand, remember that you will lose precision when converting a PHP array to an NDArray. This is generally not an issue for most use cases. "},{"title":"Multi-Type Support​","type":1,"pageTitle":"The NDArray Object","url":"/docs/ndarray#multi-type-support","content":"Currently the only supported type is the float32 type, however, NDArray has been developed internally to support multi-types in the future. We will work to implement support for other types. note We'll work on implementing smaller types like half precision floats first. "},{"title":"Broadcast","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/broadcast","content":"","keywords":""},{"title":"Scalar Broadcast​","type":1,"pageTitle":"Broadcast","url":"/docs/tutorial-basics/broadcast#scalar-broadcast","content":"Arithmetic operations involving an NDArray and a scalar value are always valid. In this case, the scalar will be &quot;expanded&quot; to the same dimensions as the NDArray. use \\NDArray as nd; // Create a matrix $array = nd::array([[1, 2], [3, 4]]); $result = $array * 2; // same as nd::multiply($array, 2); // Element-wise multiplication of $array * 2 // Same as nd:multiply($array, [[2, 2], [2, 2]]); print_r($result);  Output [[2, 4], [6, 8]]  "},{"title":"General Broadcast​","type":1,"pageTitle":"Broadcast","url":"/docs/tutorial-basics/broadcast#general-broadcast","content":"In some cases, when the format of matrices is aligned, it is also possible to perform arithmetic operations with matrices of different formats and dimensionality. Example 1​ use \\NDArray as nd; $a = nd::array( [[1, 2, 3], [4, 5, 6], [7, 8, 9]] ); $result = $a * [1, 2, 3]; echo $result;  Output [[1, 4, 9] [4, 10, 18] [7, 16, 27]]  Example 2​ use \\NDArray as nd; $a = nd::array( [[1, 2, 3], [4, 5, 6], [7, 8, 9]] ); $result = $a * [[1], [2], [3]]; echo $result;  Output [[1, 2, 3] [8, 10, 12] [21, 24, 27]]  "},{"title":"Compiling from source","type":0,"sectionRef":"#","url":"/install/from-source","content":"","keywords":""},{"title":"Requirements​","type":1,"pageTitle":"Compiling from source","url":"/install/from-source#requirements","content":"Before installing NumPower, ensure that your system meets the following requirements: PHP development files and header files. This may vary based on your operating system. Consult your system's package manager or PHP documentation for specific instructions.C++ compiler with C++11 support (such as GCC or Clang).CBLAS: CBLAS library (for CPU-based linear algebra operations)LAPACKE: LAPACKE library (for advanced linear algebra operations) Optional requirements (for enhanced performance and additional functionalities): CUBLAS: NVIDIA CUDA Toolkit with CUBLAS library (for GPU-accelerated linear algebra operations)CUDA development toolkit (NVCC) and the necessary CUDA header files "},{"title":"Compiling​","type":1,"pageTitle":"Compiling from source","url":"/install/from-source#compiling","content":"Clone the NumPower source code repository from the official GitHub repository to your local machine. Build and compile the PHP extension by running the appropriate commands specific to your environment. "},{"title":"CPU Compile​","type":1,"pageTitle":"Compiling from source","url":"/install/from-source#cpu-compile","content":"$ phpize $ ./configure $ make $ make install  "},{"title":"GPU Compile​","type":1,"pageTitle":"Compiling from source","url":"/install/from-source#gpu-compile","content":"$ phpize $ ./configure --with-cuda $ make install-cuda  Once the compilation is complete, you will obtain a compiled PHP extension file. If necessary, copy the compiled extension file to the appropriate location in your PHP extensions directory. Update your PHP configuration file (php.ini) to include the extension entry for NumPower. Add the following line to the php.ini file: extension=ndarray.so  "},{"title":"Installing NumPower","type":0,"sectionRef":"#","url":"/install/install","content":"Installing NumPower danger The NumPower library does not have a release and is in preview. This was done so that more people can test and report problems. We do not recommend using this library in production environments in its current state. Memory errors like overflow, leaks and unpredictable values ​​can happen. Please! Help us by reporting bugs in our official repository https://github.com/NumPower/numpower This page provides instructions for installing NumPower, a powerful numerical computing library for PHP inspired by the functionality of NumPy in Python. NumPower enables efficient array operations, mathematical functions, and tools for manipulating and analyzing numerical data in PHP. NumPower is implemented as a PHP extension, which requires a compilation process to install it in your PHP environment. There are two methods for installing NumPower: using a Docker image or compiling the extension from the source.","keywords":""},{"title":"NDArray Creation","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/ndarray-creation","content":"","keywords":""},{"title":"Convert PHP Array to NDArray​","type":1,"pageTitle":"NDArray Creation","url":"/docs/tutorial-basics/ndarray-creation#convert-php-array-to-ndarray","content":"In the example below we will create a matrix of size 2 x 2 from an array in PHP use \\NDArray as nd; $ndarray = new nd([[1, 2], [3, 4]]);  note Boolean casting​ Boolean values in a PHP array will be converted to floats during initialization  "},{"title":"Using initializers​","type":1,"pageTitle":"NDArray Creation","url":"/docs/tutorial-basics/ndarray-creation#using-initializers","content":"You can also use one of several available initialization methods, below we will initialize a 2 x 4 x 4 NDArrayfull of ones. &lt;?php use \\NDArray as nd; $ndarray = nd::ones([2, 4, 4]); print_r($ndarray);  Output [[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]]   "},{"title":"Creating NDArray from an image​","type":1,"pageTitle":"NDArray Creation","url":"/docs/tutorial-basics/ndarray-creation#creating-ndarray-from-an-image","content":"If you want to work with images, NumPower offers a quick way to perform image processing. You can initialize an NDArray from a GD image: &lt;?php use \\NDArray as nd; // Use GD to load the image $originalImage = imagecreatefromjpeg('test_img.jpg'); // Initialize our GD image (1200x1920) NDArray $image_array = nd::array($originalImage); print_r($image_array-&gt;shape());  Output Array ( [0] =&gt; 3 [1] =&gt; 1200 [2] =&gt; 1920 )  note For now, all images are processed in RGB format, we will work to implement other formats like CMYK and ARGB.  "},{"title":"Core","type":0,"sectionRef":"#","url":"/lattice/api/layers/core","content":"Core","keywords":""},{"title":"Convolution","type":0,"sectionRef":"#","url":"/lattice/api/layers/convolution-layers","content":"Convolution","keywords":""},{"title":"Normalization","type":0,"sectionRef":"#","url":"/lattice/api/layers/normalization-layers","content":"Normalization","keywords":""},{"title":"Indexing & Slicing","type":0,"sectionRef":"#","url":"/docs/tutorial-basics/ndarray-indexing","content":"","keywords":""},{"title":"Indexing​","type":1,"pageTitle":"Indexing & Slicing","url":"/docs/tutorial-basics/ndarray-indexing#indexing","content":"Indexing on the NDArray can be treated exactly like a standard array, with a few extras that we'll see next. The simplest way to set and return a value from an NDArray is through the standard PHP indexer. NDArray implements the ArrayAcess PHP interface. &lt;?php use \\NDArray as nd; $a = nd::zeros([10, 10]); // Initialize NDArray of shape (10, 10) full of zeros echo $a[0][0]; // Get echo &quot;\\n&quot;; $a[0][0] = 1; // Set echo $a[0][0]; // Get  Output 0 1  note The __offsetGet() method will return a slice of the root NDArray, a memory copy will only be made if the slice in question is changed. "},{"title":"Slicing​","type":1,"pageTitle":"Indexing & Slicing","url":"/docs/tutorial-basics/ndarray-indexing#slicing","content":"Python-inspired slicing can be used with NDArrays by calling the slice() method of any NDArray object. The slice is applied per dimension, and each tuple used as an argument represents the slice of a dimension. &lt;?php use \\NDArray as nd; // Create a matrix $array = nd::array([[1, 2], [3, 4]]); // Slice the second column of $array // // The first tuple [] represents ALL rows // The second tuple [1, 2] represents start: 1, stop: 2 and step: 1 (default) $second_column = $array-&gt;slice([], [1, 2]); print_r($second_column);  Output [[2], [4]]  note Negative values ​​for start, stop and step are not supported yet, but are in the roadmap. danger Although print_r works with the NDArray object, other PHP standard library array methods like array_column are not compatible with NDArrays. "},{"title":"Merging","type":0,"sectionRef":"#","url":"/lattice/api/layers/merging-layers","content":"Merging","keywords":""},{"title":"Pooling","type":0,"sectionRef":"#","url":"/lattice/api/layers/pooling-layers","content":"Pooling","keywords":""},{"title":"Reshaping","type":0,"sectionRef":"#","url":"/lattice/api/layers/reshaping-layers","content":"Reshaping","keywords":""},{"title":"Regularization","type":0,"sectionRef":"#","url":"/lattice/api/layers/regularization-layers","content":"Regularization","keywords":""},{"title":"Probabilistic","type":0,"sectionRef":"#","url":"/lattice/api/losses/probabilistic","content":"Probabilistic","keywords":""},{"title":"Model","type":0,"sectionRef":"#","url":"/lattice/api/model/model-class","content":"Model","keywords":""},{"title":"Regression","type":0,"sectionRef":"#","url":"/lattice/api/losses/regression","content":"Regression","keywords":""},{"title":"The Stack Model","type":0,"sectionRef":"#","url":"/lattice/api/model/stack","content":"The Stack Model","keywords":""},{"title":"Adadelta","type":0,"sectionRef":"#","url":"/lattice/api/optimizers/adadelta","content":"Adadelta","keywords":""},{"title":"Model Persistence","type":0,"sectionRef":"#","url":"/lattice/api/model/persistence","content":"Model Persistence","keywords":""},{"title":"Model Training","type":0,"sectionRef":"#","url":"/lattice/api/model/training","content":"Model Training","keywords":""},{"title":"Adagrad","type":0,"sectionRef":"#","url":"/lattice/api/optimizers/adagrad","content":"Adagrad","keywords":""},{"title":"Adam","type":0,"sectionRef":"#","url":"/lattice/api/optimizers/adam","content":"Adam","keywords":""},{"title":"Adafactor","type":0,"sectionRef":"#","url":"/lattice/api/optimizers/adafactor","content":"Adafactor","keywords":""},{"title":"Nadam","type":0,"sectionRef":"#","url":"/lattice/api/optimizers/nadam","content":"Nadam","keywords":""},{"title":"RMSProp","type":0,"sectionRef":"#","url":"/lattice/api/optimizers/rmsprop","content":"RMSProp","keywords":""},{"title":"SGD","type":0,"sectionRef":"#","url":"/lattice/api/optimizers/sgd","content":"SGD","keywords":""},{"title":"Introducing Lattice","type":0,"sectionRef":"#","url":"/lattice/getting-started/getting-started","content":"Introducing Lattice","keywords":""},{"title":"Installing Lattice","type":0,"sectionRef":"#","url":"/lattice/intro/lattice-install","content":"Installing Lattice","keywords":""},{"title":"What is Lattice?","type":0,"sectionRef":"#","url":"/lattice/intro/lattice-intro","content":"What is Lattice?","keywords":""},{"title":"multiply","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/multiply","content":"multiply public function multiply(int|float|array|\\NDArray|Tensor $value, string $name = ''): Tensor ","keywords":""},{"title":"divide","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/divide","content":"divide public function divide(int|float|array|\\NDArray|Tensor $value, string $name = ''): Tensor ","keywords":""},{"title":"mod","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/mod","content":"mod public function mod(int|float|array|\\NDArray|Tensor $value, string $name = ''): Tensor ","keywords":""},{"title":"add","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/add","content":"add public function add(int|float|array|\\NDArray|Tensor $value, string $name = ''): Tensor ","keywords":""},{"title":"subtract","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/subtract","content":"subtract public function subtract(int|float|array|\\NDArray|Tensor $value, string $name = ''): Tensor ","keywords":""},{"title":"negative","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/negative","content":"negative public function negative(string $name = ''): Tensor ","keywords":""},{"title":"sum","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/sum","content":"sum public function sum(bool $keepdim = false, string $name = ''): Tensor ","keywords":""},{"title":"power","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/power","content":"power public function power(int|float|array|\\NDArray|Tensor $value, string $name = ''): Tensor ","keywords":""},{"title":"exp2","type":0,"sectionRef":"#","url":"/tensor/api/exponents/exp2","content":"exp2 public function exp2(string $name = ''): Tensor ","keywords":""},{"title":"exp","type":0,"sectionRef":"#","url":"/tensor/api/exponents/exp","content":"exp public function exp(string $name = ''): Tensor ","keywords":""},{"title":"expm1","type":0,"sectionRef":"#","url":"/tensor/api/exponents/expm1","content":"expm1 public function expm1(string $name = ''): Tensor ","keywords":""},{"title":"sum_axis","type":0,"sectionRef":"#","url":"/tensor/api/arithmetics/sum_axis","content":"sum_axis public function sum_axis(int $axis, bool $keepdim = false, string $name = ''): Tensor ","keywords":""},{"title":"log","type":0,"sectionRef":"#","url":"/tensor/api/exponents/log","content":"log public function log(string $name = ''): Tensor ","keywords":""},{"title":"log10","type":0,"sectionRef":"#","url":"/tensor/api/exponents/log10","content":"log10 public function log10(string $name = ''): Tensor ","keywords":""},{"title":"log2","type":0,"sectionRef":"#","url":"/tensor/api/exponents/log2","content":"log2 public function log2(string $name = ''): Tensor ","keywords":""},{"title":"log1p","type":0,"sectionRef":"#","url":"/tensor/api/exponents/log1p","content":"log1p public function log1p(string $name = ''): Tensor ","keywords":""},{"title":"arcsinh","type":0,"sectionRef":"#","url":"/tensor/api/hyperbolic/arcsinh","content":"arcsinh public function arcsinh(string $name = ''): Tensor ","keywords":""},{"title":"arccosh","type":0,"sectionRef":"#","url":"/tensor/api/hyperbolic/arccosh","content":"arccosh public function arccosh(string $name = ''): Tensor ","keywords":""},{"title":"arctanh","type":0,"sectionRef":"#","url":"/tensor/api/hyperbolic/arctanh","content":"arctanh public function arctanh(string $name = ''): Tensor ","keywords":""},{"title":"arctan2","type":0,"sectionRef":"#","url":"/tensor/api/hyperbolic/arctan2","content":"arctan2 public function arctan2(int|float|array|\\NDArray|Tensor $y, string $name = ''): Tensor ","keywords":""},{"title":"cosh","type":0,"sectionRef":"#","url":"/tensor/api/hyperbolic/cosh","content":"cosh public function cosh(string $name = ''): Tensor ","keywords":""},{"title":"sinc","type":0,"sectionRef":"#","url":"/tensor/api/hyperbolic/sinc","content":"sinc public function sinc(string $name = ''): Tensor ","keywords":""},{"title":"sinh","type":0,"sectionRef":"#","url":"/tensor/api/hyperbolic/sinh","content":"sinh public function sinh(string $name = ''): Tensor ","keywords":""},{"title":"tanh","type":0,"sectionRef":"#","url":"/tensor/api/hyperbolic/tanh","content":"tanh public function tanh(string $name = ''): Tensor ","keywords":""},{"title":"cond","type":0,"sectionRef":"#","url":"/tensor/api/linalg/cond","content":"cond public function cond(string $name = ''): Tensor ","keywords":""},{"title":"det","type":0,"sectionRef":"#","url":"/tensor/api/linalg/det","content":"det public function det(string $name = ''): Tensor ","keywords":""},{"title":"matrix_rank","type":0,"sectionRef":"#","url":"/tensor/api/linalg/matrix_rank","content":"matrix_rank public function matrix_rank(string $name = ''): Tensor ","keywords":""},{"title":"norm","type":0,"sectionRef":"#","url":"/tensor/api/linalg/norm","content":"norm public function norm(string $name = ''): Tensor ","keywords":""},{"title":"matmul","type":0,"sectionRef":"#","url":"/tensor/api/linalg/matmul","content":"matmul public function matmul(int|float|array|\\NDArray|Tensor $value, string $name = ''): Tensor ","keywords":""},{"title":"dot","type":0,"sectionRef":"#","url":"/tensor/api/linalg/dot","content":"dot public function dot(int|float|array|\\NDArray|Tensor $y, string $name = ''): Tensor ","keywords":""},{"title":"outer","type":0,"sectionRef":"#","url":"/tensor/api/linalg/outer","content":"outer public function outer(int|float|array|\\NDArray|Tensor $vec2, string $name = ''): Tensor ","keywords":""},{"title":"reshape","type":0,"sectionRef":"#","url":"/tensor/api/manipulation/reshape","content":"reshape public function reshape(array $shape, string $name = ''): Tensor Gives a new shape to an array without changing its data.","keywords":""},{"title":"ceil","type":0,"sectionRef":"#","url":"/tensor/api/rounding/ceil","content":"ceil public function ceil(string $name = ''): Tensor ","keywords":""},{"title":"svd","type":0,"sectionRef":"#","url":"/tensor/api/linalg/svd","content":"svd public function svd(string $name = ''): Tensor ","keywords":""},{"title":"floor","type":0,"sectionRef":"#","url":"/tensor/api/rounding/floor","content":"floor public function floor(string $name = ''): Tensor ","keywords":""},{"title":"clip","type":0,"sectionRef":"#","url":"/tensor/api/rounding/clip","content":"clip public function clip(float $min, float $max, string $name = ''): Tensor ","keywords":""},{"title":"trunc","type":0,"sectionRef":"#","url":"/tensor/api/rounding/trunc","content":"trunc public function trunc(string $name = ''): Tensor ","keywords":""},{"title":"backward","type":0,"sectionRef":"#","url":"/tensor/api/tensor/backward","content":"backward public function backward(\\NDArray|float|int $grad = null): void Computes the gradient of current tensor graph leaves.","keywords":""},{"title":"detach","type":0,"sectionRef":"#","url":"/tensor/api/tensor/detach","content":"detach public function detach(): Tensor Returns a new Tensor, detached from the current graph. The result will never require gradient.","keywords":""},{"title":"getName","type":0,"sectionRef":"#","url":"/tensor/api/tensor/getName","content":"getName public function getName(): string ","keywords":""},{"title":"getData","type":0,"sectionRef":"#","url":"/tensor/api/tensor/getData","content":"getData public function getData(): \\NDArray|float ","keywords":""},{"title":"getShape","type":0,"sectionRef":"#","url":"/tensor/api/tensor/getShape","content":"getShape public function getShape(): array|int Returns the shape of the Tensor or 0 if the Tensor is a scalar.","keywords":""},{"title":"grad","type":0,"sectionRef":"#","url":"/tensor/api/tensor/grad","content":"grad public function grad(): Tensor This attribute is NULL by default and becomes a Tensor the first time a call to backward() computes gradients for self. The attribute will then contain the gradients computed and future calls to backward() will accumulate (add) gradients into it.","keywords":""},{"title":"getTape","type":0,"sectionRef":"#","url":"/tensor/api/tensor/getTape","content":"getTape public function getTape(): ?GradientTape Get current gradient tape head for self.","keywords":""},{"title":"graph","type":0,"sectionRef":"#","url":"/tensor/api/tensor/graph","content":"graph public function graph(): void Print the Tensor graph.","keywords":""},{"title":"isGPU","type":0,"sectionRef":"#","url":"/tensor/api/tensor/isGPU","content":"isGPU public function isGPU(): bool ","keywords":""},{"title":"numElements","type":0,"sectionRef":"#","url":"/tensor/api/tensor/numElements","content":"numElements public function numElements(): int Returns the total number of elements in the tensor.","keywords":""},{"title":"requireGrad","type":0,"sectionRef":"#","url":"/tensor/api/tensor/requireGrad","content":"requireGrad public function requireGrad(): bool ","keywords":""},{"title":"isScalar","type":0,"sectionRef":"#","url":"/tensor/api/tensor/isScalar","content":"isScalar public function isScalar(): bool Returns true if the tensor is a scalar or false if it is an n-dimensional array.","keywords":""},{"title":"resetGradients","type":0,"sectionRef":"#","url":"/tensor/api/tensor/resetGradients","content":"resetGradients public function resetGradients(): void ","keywords":""},{"title":"arcsin","type":0,"sectionRef":"#","url":"/tensor/api/trigonometric/arcsin","content":"arcsin public function arcsin(string $name = ''): Tensor ","keywords":""},{"title":"toArray","type":0,"sectionRef":"#","url":"/tensor/api/tensor/toArray","content":"toArray public function toArray(): array|float Converts the buffer data to a PHP object, array or scalar.","keywords":""},{"title":"acos","type":0,"sectionRef":"#","url":"/tensor/api/trigonometric/acos","content":"acos public function acos(string $name = ''): Tensor ","keywords":""},{"title":"arctan","type":0,"sectionRef":"#","url":"/tensor/api/trigonometric/arctan","content":"arctan public function arctan(string $name = ''): Tensor ","keywords":""},{"title":"cos","type":0,"sectionRef":"#","url":"/tensor/api/trigonometric/cos","content":"cos public function cos(string $name = ''): Tensor ","keywords":""},{"title":"sin","type":0,"sectionRef":"#","url":"/tensor/api/trigonometric/sin","content":"sin public function sin(string $name = ''): Tensor ","keywords":""},{"title":"radians","type":0,"sectionRef":"#","url":"/tensor/api/trigonometric/radians","content":"radians public function radians(string $name = ''): Tensor ","keywords":""},{"title":"tan","type":0,"sectionRef":"#","url":"/tensor/api/trigonometric/tan","content":"tan public function tan(string $name = ''): Tensor ","keywords":""},{"title":"What is Automatic Differentiation (Autograd)?","type":0,"sectionRef":"#","url":"/tensor/intro/autograd-intro","content":"","keywords":""},{"title":"Uses of Autograd​","type":1,"pageTitle":"What is Automatic Differentiation (Autograd)?","url":"/tensor/intro/autograd-intro#uses-of-autograd","content":"Automatic differentiation has a broad spectrum of uses across various fields: Machine Learning: Autograd is essential in training neural networks. During backpropagation, autograd computes the gradients of the loss function with respect to the model parameters, enabling the optimization algorithms to update the weights effectively. Optimization: In optimization problems, gradients provide crucial information about the direction in which a function increases or decreases. Autograd helps in finding the optimal points of complex, high-dimensional functions. Scientific Computing: Many scientific simulations require the computation of derivatives to understand sensitivities and to perform tasks like solving differential equations. Autograd simplifies these computations and enhances accuracy. Physics and Engineering: Autograd is used in physics-based simulations and engineering applications to compute forces, velocities, and other derivative-dependent properties. Economics and Finance: In quantitative finance, autograd aids in computing sensitivities of financial models (like Greeks in option pricing) and in optimizing economic models. "},{"title":"Installing Autograd","type":0,"sectionRef":"#","url":"/tensor/intro/installing","content":"","keywords":""},{"title":"Requirements​","type":1,"pageTitle":"Installing Autograd","url":"/tensor/intro/installing#requirements","content":"NumPower Extension &gt;= 0.5.0 (https://www.numpower.org/install/from-source)PHP &gt;= 8.3Composer "},{"title":"Installing​","type":1,"pageTitle":"Installing Autograd","url":"/tensor/intro/installing#installing","content":"$ composer require numpower/autograd  "},{"title":"ArithmeticOperand Class","type":0,"sectionRef":"#","url":"/tensor/low-level/arithmetic-operand","content":"ArithmeticOperand Class The abstract class ArithmeticOperand allows PHP classes that extend it to implement magic methods for native PHP arithmetic operations. class CustomOperand extends \\ArithmeticOperand { public function __add(int|float|array|object $b); public function __mul(int|float|array|object $b); public function __pow(int|float|array|object $b); public function __div(int|float|array|object $b); public function __sub(int|float|array|object $b); public function __mod(int|float|array|object $b); } Using the example above, PHP will execute the code below: $a = new CustomOperand(); $c = $a * 2; ","keywords":""},{"title":"Losses","type":0,"sectionRef":"#","url":"/tensor/nn/losses","content":"","keywords":""},{"title":"Regression​","type":1,"pageTitle":"Losses","url":"/tensor/nn/losses#regression","content":" Regression losses are used to evaluate the performance of regression models, which predict continuous values. These losses quantify the difference between the predicted values generated by the model and the actual values from the data. The goal during training is to minimize these losses, thereby improving the model's accuracy. "},{"title":"MeanSquaredError​","type":1,"pageTitle":"Losses","url":"/tensor/nn/losses#meansquarederror","content":"public static function MeanSquaredError(int|float|array|\\NDArray|Tensor $x, int|float|array|\\NDArray|Tensor $y, ?string $reduction = 'mean', string $name = ''): Tensor  Calculates the Mean Squared Error (MSE) between two inputs, $x and $y. This function is a key metric for evaluating the performance of regression models. The MSE is computed by averaging the squared differences between the predicted and actual values. An optional $reduction parameter can be specified to control how the final result is aggregated. By default, it is set to mean, which returns the average of all squared errors. Another common option is sum, which returns the total sum of all squared errors. "},{"title":"MeanAbsoluteError​","type":1,"pageTitle":"Losses","url":"/tensor/nn/losses#meanabsoluteerror","content":"public static function MeanAbsoluteError(int|float|array|\\NDArray|Tensor $x, int|float|array|\\NDArray|Tensor $y, ?string $reduction = 'mean', string $name = ''): Tensor  Calculates the Mean Absolute Error (MAE) between two inputs, $x and $y. This function is essential for assessing the accuracy of regression models by measuring the average magnitude of the errors in a set of predictions, without considering their direction. The MAE is computed by averaging the absolute differences between the predicted and actual values. The $reduction parameter allows customization of how the final result is summarized. By default, it is set to mean, which returns the average of all absolute errors, but sumcan also be specified to obtain the total sum of all absolute errors. "},{"title":"Probabilistic​","type":1,"pageTitle":"Losses","url":"/tensor/nn/losses#probabilistic","content":" Probabilistic losses are used in models that predict probability distributions over outcomes rather than single point estimates. These losses measure how well the predicted probability distribution aligns with the actual distribution of the data. The goal is to minimize these losses to improve the model's ability to predict accurate probabilities. "},{"title":"BinaryCrossEntropy​","type":1,"pageTitle":"Losses","url":"/tensor/nn/losses#binarycrossentropy","content":"public static function BinaryCrossEntropy(int|float|array|\\NDArray|Tensor $x, int|float|array|\\NDArray|Tensor $y, ?string $reduction = 'mean', string $name = ''): Tensor  Computes the Binary Cross Entropy loss between the inputs $x (predictions) and $y (targets). This function is commonly used in binary classification tasks to measure the difference between probability distributions of predicted and actual class labels. The $reduction parameter specifies how the final loss should be aggregated. The default is mean, which returns the average loss across all samples. Alternatively, sum can be specified to return the total sum of losses. "},{"title":"Basic usage","type":0,"sectionRef":"#","url":"/tensor/intro/basic-usage","content":"","keywords":""},{"title":"The Tensor class​","type":1,"pageTitle":"Basic usage","url":"/tensor/intro/basic-usage#the-tensor-class","content":"A Tensor is a fundamental data structure used in machine learning and deep learning to represent multidimensional arrays. Tensors generalize matrices to higher dimensions. They are essentially a way to store data in a format that can be efficiently processed by various mathematical operations and are crucial in the implementation of neural networks and other machine learning algorithms. &lt;?php require_once &quot;vendor/autoload.php&quot;; use \\NumPower\\Tensor; $x = new Tensor([[1, 2], [3, 4]]); $y = new Tensor([[5, 6], [7, 8]]);  In the code above we created two tensors, and now we can perform several high-performance mathematical operations, let's do some mathematical operations: $result = ($x + $y) / $y; echo $result;  [[1.2, 1.33333] [1.42857, 1.5]]  "},{"title":"Tensor naming and graphs​","type":1,"pageTitle":"Basic usage","url":"/tensor/intro/basic-usage#tensor-naming-and-graphs","content":"Before calculating the derivatives, let's call the graph method so we can visualize our backward pass, this step is optional and is generally used for debugging: $result-&gt;graph();  Operation Arguments ==================== ======================================== divide [_nd_, _nd_] add [_nd_, _nd_]  We can see that our operations were properly registered, but as our tensors are not named, they only appear as _nd_in the arguments. To make the graph and other parts of your code easier to read, we can name our tensors: $x = new Tensor([[1, 2], [3, 4]], name: 'x'); $y = new Tensor([[5, 6], [7, 8]], name: 'y');  Now when we call the graph() method we will be able to visualize more clearly our forward pass (from bottom to top) and our backward pass (from top to bottom) where the derivatives will be calculated. Operation Arguments ==================== ======================================== divide [x, y] add [x, y]  The output tensors will inherit the names of the input tensors in some operations, so x in the division operation is actually the result of the sum of x + y. You override this name in operations when specifying a name for the output of an operation using the name argument $result = $x-&gt;add($y, name: 'out_add') / $y;  Now if we run the code again, we will have the following graph: Operation Arguments ==================== ======================================== divide [out_add, y] add [x, y]  Great, now we have a clear view of our graph and an overview of our forward and backward pass. "},{"title":"Calculating gradients​","type":1,"pageTitle":"Basic usage","url":"/tensor/intro/basic-usage#calculating-gradients","content":"Finally, let's now calculate the gradients of $result with respect to $x and $y. To do this, simply call the backward() method in the $result variable. $result-&gt;backward();  Fatal error: Uncaught Exception: grad can only be created for scalar outputs  Oops! We have an exception. This happens because the Autograd library requires the gradient to be calculated from a scalar and in this case, $result is an n-dimensional array. To do this, we need to reduce our output using a method like sum or mean: $result = ($x-&gt;add($y, name: 'out_add') / $y)-&gt;sum();  Now that we have a scalar, when we call Tensor's backward() method the derivatives can be calculated. To get the gradients of $x and $y, just use the grad() method on both variables after calling the backward method: $result-&gt;backward(); echo $x-&gt;grad(); echo $y-&gt;grad();  Fatal error: Uncaught Exception: No gradient found for `x`  Another exception, but now our problem is that we do not specify which tensors should have their gradients calculated. By default, tensors do not require gradient calculation, this is to avoid calculating and storing useless gradients in more complex functions. We will use the requireGrad argument when creating $x and $y to say that we would like to calculate the gradients for both input tensors: $x = new Tensor([[1, 2], [3, 4]], name: 'x', requireGrad: True); $y = new Tensor([[5, 6], [7, 8]], name: 'y', requireGrad: True);  Now when we re-execute the code, we will have the gradients of both inputs in relation to $result: [[0.2, 0.166667] [0.142857, 0.125]] [[-0.04, -0.0555556] [-0.0612245, -0.0625]]  "},{"title":"Section source code​","type":1,"pageTitle":"Basic usage","url":"/tensor/intro/basic-usage#section-source-code","content":"Here is what your code should look like when you finish this section: &lt;?php require_once &quot;vendor/autoload.php&quot;; use NumPower\\Tensor; $x = new Tensor([[1, 2], [3, 4]], name: 'x', requireGrad: True); $y = new Tensor([[5, 6], [7, 8]], name: 'y', requireGrad: True); $result = ($x-&gt;add($y, name: 'out_add') / $y)-&gt;sum(); $result-&gt;backward(); echo $x-&gt;grad(); echo $y-&gt;grad();  "},{"title":"Using the GPU to perform operations​","type":1,"pageTitle":"Basic usage","url":"/tensor/intro/basic-usage#using-the-gpu-to-perform-operations","content":"If you have a graphics card with CUDA support and have compiled and installed the NumPower extension with GPU support enabled, you can use your GPU to store and perform mathematical and manipulation operations. In our implementation, we thought about practicality and therefore, to use your GPU, simply use theuseGpu argument when creating your Tensor. $x = new Tensor([[1, 2], [3, 4]], name: 'x', requireGrad: True, useGpu: True); $y = new Tensor([[5, 6], [7, 8]], name: 'y', requireGrad: True, useGpu: True);  Device Mismatch: Like NDArrays, tensors also require that all tensors involved in operations with multiple arguments are stored on the same device. Operations between tensors stored on the CPU and others stored on the GPU will cause a fatal error. "},{"title":"Activations","type":0,"sectionRef":"#","url":"/tensor/nn/activations","content":"","keywords":""},{"title":"Non-Linear Activations​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#non-linear-activations","content":" "},{"title":"tanh​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#tanh","content":"public static function tanh(int|float|array|\\NDArray|Tensor $x, string $name = 'out_tanh'): Tensor  This function computes the hyperbolic tangent (tanh) activation function on the provided input. Tanh transforms each element of the input tensor using the hyperbolic tangent function, which maps values to the range (-1, 1). It is useful for normalizing inputs in neural networks and other computational models, providing a smooth transition between negative and positive values. "},{"title":"softplus​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#softplus","content":"public static function softplus(int|float|array|\\NDArray|Tensor $x, string $name = 'out_softplus'): Tensor  This function computes the Softplus activation function on the provided input. Softplus transforms each element of the input tensor using the natural logarithm of the sum of the exponential of the element and one. This operation smooths out the output, ensuring it is always positive. The resulting tensor, with the same shape as the input, represents the output of the softplus operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_softplus'. "},{"title":"softmax​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#softmax","content":"public static function softmax(int|float|array|\\NDArray|Tensor $x, string $name = 'out_softmax'): Tensor  This function computes the softmax activation function on the provided input. Softmax transforms each element of the input tensor into a probability distribution by exponentiating each element and then normalizing the tensor so that the sum of all elements equals one. This makes softmax suitable for multi-class classification tasks, where it outputs probabilities for each class. The resulting tensor, with the same shape as the input, represents the output of the softmax operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_softmax'. "},{"title":"softsign​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#softsign","content":"public static function softsign(int|float|array|\\NDArray|Tensor $x, string $name = 'out_softsign'): Tensor  This function computes the Softsign activation function on the provided input. The softsign function transforms each element of the input tensor by dividing it by the absolute value of itself plus one. This transformation maps the input values to the range (-1, 1), preserving zero. The resulting tensor, with the same shape as the input, represents the output of the softsign operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_sigmoid'. "},{"title":"ReLU​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#relu","content":"public static function ReLU(int|float|array|\\NDArray|Tensor $inputs, string $name = 'out_relu'): Tensor  This function implements the Rectified Linear Unit (ReLU) activation function for a given tensor of inputs. ReLU activation sets all negative values in the tensor to zero, leaving positive values unchanged. The function returns a tensor with the same shape as the input tensor, representing the output of the ReLU operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_relu'. "},{"title":"CELU​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#celu","content":"public static function CELU(int|float|array|\\NDArray|Tensor $x, float $alpha = 1.0, string $name = 'out_celu'): Tensor  This function computes the Continuous Exponential Linear Unit (CELU) activation function on the provided input. CELU applies a non-linear transformation that smooths negative values of the input tensor based on the parameter $alpha, while leaving positive values unchanged. The resulting tensor, with the same shape as the input, represents the output of the CELU operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_celu'. "},{"title":"SiLU​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#silu","content":"public static function SiLU(int|float|array|\\NDArray|Tensor $x, float $beta = 1.0, string $name = 'out_silu'): Tensor  This function computes the Sigmoid-weighted Linear Unit (SiLU), also known as the Swish activation function, on the provided input. SiLU applies a non-linear transformation that smooths negative values of the input tensor based on the parameter $beta, using a sigmoid function, while preserving positive values unchanged. The resulting tensor, with the same shape as the input, represents the output of the SiLU operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_silu'. "},{"title":"SELU​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#selu","content":"public static function SELU(int|float|array|\\NDArray|Tensor $inputs, float $alpha=1.67326, float $scale=1.0507, string $name = 'out_selu'): Tensor  This function applies the Scaled Exponential Linear Unit (SELU) activation function to a tensor of inputs. SELU applies a scaled version of the Exponential Linear Unit (ELU), transforming each element of the input tensor based on the parameters $alpha and $scale. The function returns a tensor with the same shape as the input tensor, representing the output of the SELU operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_selu'. "},{"title":"exponential​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#exponential","content":"public static function exponential(int|float|array|\\NDArray|Tensor $x, string $name = 'out_exponential'): Tensor  This function computes the exponential function on the provided input. The exponential function raises the mathematical constant 𝑒 (approximately 2.718) to the power of each element in the input tensor. This operation is commonly used in various mathematical and statistical computations. The resulting tensor, with the same shape as the input, represents the output of the exponential operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_exponential'. "},{"title":"mish​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#mish","content":"public static function mish(int|float|array|\\NDArray|Tensor $x, string $name = 'out_mish'): Tensor  This function computes the Mish activation function on the provided input. Mish is a relatively newer activation function that smooths and enhances the training of neural networks by introducing a non-linearity that is differentiable and has favorable properties during gradient descent. "},{"title":"sigmoid​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#sigmoid","content":"public static function sigmoid(int|float|array|\\NDArray|Tensor $x, string $name = 'out_sigmoid'): Tensor  This function computes the sigmoid activation function on the provided input. The sigmoid function transforms each element of the input tensor to a value between 0 and 1, representing the probability-like output of a binary classification decision. The resulting tensor, with the same shape as the input, represents the output of the sigmoid operation. The optional parameter $name specifies the name of the output tensor and defaults to 'out_sigmoid'. "},{"title":"Linear Activations​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#linear-activations","content":" A linear activation function, also known as an identity activation function, is a function where the output is directly proportional to the input. In mathematical terms, if the input to the function is 𝑥, the output is also 𝑥. "},{"title":"Linear​","type":1,"pageTitle":"Activations","url":"/tensor/nn/activations#linear","content":"public static function linear(int|float|array|\\NDArray|Tensor $x, string $name = 'out_linear'): Tensor  This function represents the identity or linear activation function, which simply returns the input tensor 𝑥 unchanged. It is used when no activation function is desired or required in a specific layer of a neural network or any other computational graph. "},{"title":"Simple Neural Net from scratch using Autograd","type":0,"sectionRef":"#","url":"/tensor/intro/autograd-model","content":"","keywords":""},{"title":"Create the SimpleModel class​","type":1,"pageTitle":"Simple Neural Net from scratch using Autograd","url":"/tensor/intro/autograd-model#create-the-simplemodel-class","content":"Let's start by creating a class that initializes the parameters we need for our neural network. Our neural network will have 1 hidden layer and 1 output layer. For our constructor, we will need the following parameters: inputDim: Dimensionality of the input features.outputDim: Dimensionality of the output.hiddenSize: Number of neurons in the hidden layer.learningRate: Learning rate for the optimizer. &lt;?php use NDArray as nd; use NumPower\\Tensor; use NumPower\\NeuralNetwork\\Activations as activation; use NumPower\\NeuralNetwork\\Losses as loss; class SimpleModel { public Tensor $weights_hidden_layer; public Tensor $weights_output_layer; public Tensor $hidden_bias; public Tensor $output_bias; private float $learningRate; public function __construct(int $inputDim = 2, int $outputDim = 1, int $hiddenSize = 16, float $learningRate = 0.01 ) { $this-&gt;learningRate = $learningRate; // Initialize hidden layer weights $this-&gt;weights_hidden_layer = new Tensor( nd::uniform([$inputDim, $hiddenSize], -0.5, 0.5), name: 'weights_hidden_layer', requireGrad: True ); // Initialize output layer weights $this-&gt;weights_output_layer = new Tensor( nd::uniform([$hiddenSize, $outputDim],-0.5, 0.5), name: 'weights_output_layer', requireGrad: True ); // Initialize hidden layer bias $this-&gt;hidden_bias = new Tensor( nd::uniform([$hiddenSize], -0.5, 0.5), name: 'hidden_bias', requireGrad: True ); // Initialize output layer bias $this-&gt;output_bias = new Tensor( nd::uniform([$outputDim], -0.5, 0.5), name: 'output_bias', requireGrad: True ); } }  For each weight and bias of our layers, we will initialize a Tensor using a random uniform algorithm from the NumPower extension. "},{"title":"Forward pass function​","type":1,"pageTitle":"Simple Neural Net from scratch using Autograd","url":"/tensor/intro/autograd-model#forward-pass-function","content":"The forward pass in a neural network is the process where the input data is passed through the network's layers to produce an output. This involves several key operations, including linear transformations (matrix multiplications), adding biases, and applying activation functions to introduce non-linearity. The forward function in the SimpleModel class is responsible for computing the predictions of the neural network as well as the loss. public function forward(Tensor $x, Tensor $y): array { // Forward pass - Hidden Layer $x = $x-&gt;matmul($this-&gt;weights_hidden_layer) + $this-&gt;hidden_bias; $x = activation::ReLU($x); // ReLU Activation // Forward pass - Output Layer $x = $x-&gt;matmul($this-&gt;weights_output_layer) + $this-&gt;output_bias; $x = activation::sigmoid($x); // Sigmoid Activation // Binary Cross Entropy Loss $loss = loss::BinaryCrossEntropy($x, $y, name: 'loss'); return [$x, $loss]; }  "},{"title":"Backward pass function​","type":1,"pageTitle":"Simple Neural Net from scratch using Autograd","url":"/tensor/intro/autograd-model#backward-pass-function","content":"This function, backward, performs the backward pass in a neural network training process, which includes the backpropagation of errors and the update of network weights and biases using Stochastic Gradient Descent (SGD). public function backward(Tensor $loss) { // Trigger autograd $loss-&gt;backward(); // SGD (Optimizer) - Update Hidden Layer weights and bias $dw_dLoss = $this-&gt;weights_hidden_layer-&gt;grad(); $this-&gt;weights_hidden_layer -= ($dw_dLoss * $this-&gt;learningRate); $this-&gt;weights_hidden_layer-&gt;resetGradients(); $this-&gt;hidden_bias -= ($this-&gt;hidden_bias-&gt;grad() * $this-&gt;learningRate); $this-&gt;hidden_bias-&gt;resetGradients(); // SGD (Optimizer) - Update Output Layer weights and bias $db_dLoss = $this-&gt;weights_output_layer-&gt;grad(); $this-&gt;weights_output_layer -= ($db_dLoss * $this-&gt;learningRate); $this-&gt;weights_output_layer-&gt;resetGradients(); $this-&gt;output_bias -= $this-&gt;output_bias-&gt;grad() * $this-&gt;learningRate; $this-&gt;output_bias-&gt;resetGradients(); }  Trigger Autograd for Loss Tensor:​ The function starts by invoking the backward method on the loss tensor. This initiates the backpropagation process, calculating the gradients of the loss with respect to all the parameters (weights and biases) in the network. Update Hidden Layer Weights and Bias:​ Retrieve Gradients: The gradient of the loss with respect to the hidden layer weights is obtained using the grad() method.Update Weights: The hidden layer weights are updated by subtracting the product of the gradients and the learning rate from the current weights.Reset Gradients: The gradients for the hidden layer weights are reset to zero to prevent accumulation in the next backward pass.Update Bias: Similarly, the hidden layer biases are updated by subtracting the product of their gradients and the learning rate.Reset Gradients: The gradients for the hidden layer biases are reset. Update Output Layer Weights and Bias:​ Retrieve Gradients: The gradient of the loss with respect to the output layer weights is obtained.Update Weights: The output layer weights are updated by subtracting the product of the gradients and the learning rate from the current weights.Reset Gradients: The gradients for the output layer weights are reset to zero.Update Bias: The output layer biases are updated by subtracting the product of their gradients and the learning rate.Reset Gradients: The gradients for the output layer biases are reset. This function ensures that the neural network parameters (weights and biases) are adjusted in response to the error calculated from the predictions, which helps the model to learn and improve its accuracy over time. The use of autograd simplifies the gradient computation process, while the manual updates and gradient resets ensure the parameters are correctly adjusted for each training iteration. "},{"title":"Training our model​","type":1,"pageTitle":"Simple Neural Net from scratch using Autograd","url":"/tensor/intro/autograd-model#training-our-model","content":"This model is already sufficient to solve several binary problems. For simplicity, let's train our model to solve the XOR problem. For this, we will use 4000 epochs during training: $num_epochs = 4000; $x = new Tensor(nd::array([[0, 0], [1, 0], [1, 1], [0, 1]]), name: 'x'); $y = new Tensor(nd::array([[0], [1], [0], [1]]), name: 'y'); $model = new SimpleModel(); for ($current_epoch = 0; $current_epoch &lt; $num_epochs; $current_epoch++) { // Forward Pass [$prediction, $loss] = $model-&gt;forward($x, $y); // Backward Pass $model-&gt;backward($loss); echo &quot;\\n Epoch ($current_epoch): &quot;.$loss-&gt;getArray(); }  "},{"title":"Predicting​","type":1,"pageTitle":"Simple Neural Net from scratch using Autograd","url":"/tensor/intro/autograd-model#predicting","content":"With our trained model, we can predict the XOR problem by performing another forward pass and checking the output of this pass: echo &quot;\\nPredicted:\\n&quot;; $predicted = $model-&gt;forward($x, $y)[0]; echo $predicted;  We can see that our neural network has converged and presents consistent results: Predicted: [[0.102802] [0.876796] [0.0873984] [0.884605]]  To use your results and tensors natively in PHP in conjunction with other libraries, you probably want to convert your Tensor to a native PHP value. To do this, simply call the toArray() method: print_r($predicted-&gt;toArray());  Array ( [0] =&gt; Array ( [0] =&gt; 0.102802 ) [1] =&gt; Array ( [0] =&gt; 0.876796 ) [2] =&gt; Array ( [0] =&gt; 0.0873984 ) [3] =&gt; Array ( [0] =&gt; 0.884605 ) )  "},{"title":"Full implementation​","type":1,"pageTitle":"Simple Neural Net from scratch using Autograd","url":"/tensor/intro/autograd-model#full-implementation","content":"Finally, this is the complete implementation of our solution. Try updating your tensors to use the GPU using the useGpu: True argument when creating your weights and biases. &lt;?php require_once &quot;vendor/autoload.php&quot;; use NDArray as nd; use NumPower\\Tensor; use NumPower\\NeuralNetwork\\Activations as activation; use NumPower\\NeuralNetwork\\Losses as loss; class SimpleModel { public Tensor $weights_hidden_layer; public Tensor $weights_output_layer; public Tensor $hidden_bias; public Tensor $output_bias; private float $learningRate; public function __construct(int $inputDim = 2, int $outputDim = 1, int $hiddenSize = 16, float $learningRate = 0.01 ) { $this-&gt;learningRate = $learningRate; // Initialize hidden layer weights $this-&gt;weights_hidden_layer = new Tensor( nd::uniform([$inputDim, $hiddenSize], -0.5, 0.5), name: 'weights_hidden_layer', requireGrad: True ); // Initialize output layer weights $this-&gt;weights_output_layer = new Tensor( nd::uniform([$hiddenSize, $outputDim],-0.5, 0.5), name: 'weights_output_layer', requireGrad: True ); // Initialize hidden layer bias $this-&gt;hidden_bias = new Tensor( nd::uniform([$hiddenSize], -0.5, 0.5), name: 'hidden_bias', requireGrad: True ); // Initialize output layer bias $this-&gt;output_bias = new Tensor( nd::uniform([$outputDim], -0.5, 0.5), name: 'output_bias', requireGrad: True ); } public function forward(Tensor $x, Tensor $y): array { // Forward pass - Hidden Layer $x = $x-&gt;matmul($this-&gt;weights_hidden_layer) + $this-&gt;hidden_bias; $x = activation::ReLU($x); // ReLU Activation // Forward pass - Output Layer $x = $x-&gt;matmul($this-&gt;weights_output_layer) + $this-&gt;output_bias; $x = activation::sigmoid($x); // Sigmoid Activation // Mean Squared Error $loss = loss::MeanSquaredError($x, $y, name: 'loss'); return [$x, $loss]; } public function backward(Tensor $loss) { // Trigger autograd $loss-&gt;backward(); // SGD (Optimizer) - Update Hidden Layer weights and bias $dw_dLoss = $this-&gt;weights_hidden_layer-&gt;grad(); $this-&gt;weights_hidden_layer -= ($dw_dLoss * $this-&gt;learningRate); $this-&gt;weights_hidden_layer-&gt;resetGradients(); $this-&gt;hidden_bias -= ($this-&gt;hidden_bias-&gt;grad() * $this-&gt;learningRate); $this-&gt;hidden_bias-&gt;resetGradients(); // SGD (Optimizer) - Update Output Layer weights and bias $db_dLoss = $this-&gt;weights_output_layer-&gt;grad(); $this-&gt;weights_output_layer -= ($db_dLoss * $this-&gt;learningRate); $this-&gt;weights_output_layer-&gt;resetGradients(); $this-&gt;output_bias -= ($this-&gt;output_bias-&gt;grad() * $this-&gt;learningRate); $this-&gt;output_bias-&gt;resetGradients(); } } $num_epochs = 4000; $x = new Tensor(nd::array([[0, 0], [1, 0], [1, 1], [0, 1]]), name: 'x'); $y = new Tensor(nd::array([[0], [1], [0], [1]]), name: 'y'); $model = new SimpleModel(); $start = microtime(true); for ($current_epoch = 0; $current_epoch &lt; $num_epochs; $current_epoch++) { // Forward Pass [$prediction, $loss] = $model-&gt;forward($x, $y); // Backward Pass $model-&gt;backward($loss); echo &quot;\\n Epoch ($current_epoch): &quot;.$loss-&gt;getArray(); } echo &quot;\\nPredicted:\\n&quot;; $predicted = $model-&gt;forward($x, $y)[0]; print_r($predicted-&gt;toArray());  "}]